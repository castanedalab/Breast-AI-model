{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f0ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80c8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selección de frames basados en área de máscara\n",
    "def select_candidate_frames(mask_npy_path, n_samples=5, tol=0.2):\n",
    "    \"\"\"\n",
    "    - Carga el .npy con máscaras (shape: D x H x W).\n",
    "    - Calcula el área (número de pixeles != 0) por frame.\n",
    "    - Encuentra el índice del frame de área máxima.\n",
    "    - Selecciona aleatoriamente hasta n_samples frames adicionales\n",
    "      cuyo área >= (1 - tol) * área_máxima.\n",
    "    - Devuelve lista de índices [idx_max, idx1, idx2, ...].\n",
    "    \"\"\"\n",
    "    masks = np.load(mask_npy_path)  # (D, H, W)\n",
    "    # Área = conteo de pixeles > 0\n",
    "    areas = (masks > 0).reshape(masks.shape[0], -1).sum(axis=1)\n",
    "    idx_max = int(np.argmax(areas))\n",
    "    max_area = areas[idx_max]\n",
    "\n",
    "    # candidatos con área suficiente (excluyendo el máximo)\n",
    "    eligible = [i for i, a in enumerate(areas)\n",
    "                if i != idx_max and a >= (1 - tol) * max_area]\n",
    "    # muestreo aleatorio\n",
    "    sampled = random.sample(eligible, min(len(eligible), n_samples))\n",
    "    return [idx_max] + sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e60a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Función para leer sólo los frames necesarios de un video\n",
    "def load_frames_from_video(video_path, frame_indices):\n",
    "    \"\"\"\n",
    "    - Usa cv2.VideoCapture para leer sólo los índices indicados.\n",
    "    - Devuelve lista de arrays (H x W x C).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = {}\n",
    "    for idx in sorted(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            raise RuntimeError(f\"Error leyendo frame {idx} de {video_path}\")\n",
    "        # convertir BGR → RGB\n",
    "        frames[idx] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cap.release()\n",
    "    # devolver en orden original de indices\n",
    "    return [frames[i] for i in frame_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e2dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset para clasificación\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, frame_list, transform=None, patient_id=None):\n",
    "        \"\"\"\n",
    "        frame_list: lista de np.array HxWxC\n",
    "        \"\"\"\n",
    "        self.frames = frame_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.frames[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0, 0  # (image, dummy_label, dummy_patient_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pipeline de inferencia\n",
    "def run_classification_on_npy_masks(\n",
    "    mask_dir, video_dir, model_list, device, transform,\n",
    "    n_per_patient=5, tol=0.2, batch_size=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Para cada paciente:\n",
    "      1. Leer mask.npy y video.mp4 (o .avi).\n",
    "      2. Seleccionar índices de frames.\n",
    "      3. Leer esos frames.\n",
    "      4. Crear DataLoader y pasar por ensemble_predictions.\n",
    "    Devuelve dict paciente → etiqueta final.\n",
    "    \"\"\"\n",
    "    from your_module import ensemble_predictions  # tu función de ensemble\n",
    "\n",
    "    results = {}\n",
    "    for fname in os.listdir(mask_dir):\n",
    "        if not fname.endswith(\".npy\"):\n",
    "            continue\n",
    "        patient_id = os.path.splitext(fname)[0]\n",
    "        mask_path  = os.path.join(mask_dir, fname)\n",
    "        video_path = os.path.join(video_dir, patient_id + \".mp4\")\n",
    "\n",
    "        # 1. seleccionar frames\n",
    "        idxs = select_candidate_frames(mask_path, n_per_patient, tol)\n",
    "        # 2. cargar imágenes\n",
    "        frames = load_frames_from_video(video_path, idxs)\n",
    "        # 3. dataset + dataloader\n",
    "        ds = FrameDataset(frames, transform=transform)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        # 4. inferencia ensemble\n",
    "        pred = ensemble_predictions(model_list, dl, device, method=\"average\")\n",
    "        results[patient_id] = pred  # asume un único valor por paciente\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # cargar modelos (adaptar a tu load_model)\n",
    "    from your_module import load_model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    models = []\n",
    "    for ckpt in [\"densenet.ckpt\",\"mobilenet.ckpt\",\"vgg16.ckpt\"]:\n",
    "        model,_ = load_model(ckpt, model_name=os.path.splitext(ckpt)[0])\n",
    "        models.append(model)\n",
    "\n",
    "    # transformaciones\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "    ])\n",
    "\n",
    "    preds = run_classification_on_npy_masks(\n",
    "        mask_dir=\"masks_npy\",\n",
    "        video_dir=\"videos\",\n",
    "        model_list=models,\n",
    "        device=device,\n",
    "        transform=transform,\n",
    "        n_per_patient=5,\n",
    "        tol=0.2,\n",
    "        batch_size=8\n",
    "    )\n",
    "    print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
