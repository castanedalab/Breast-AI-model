{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350d9eff",
   "metadata": {},
   "source": [
    "# Test de metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cfa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Patient 2] 4 pares válidos\n",
      "[Patient 3] 4 pares válidos\n",
      "El recorte del GT (393, 1047, 317) no coincide con el del video (393, 1075, 865, 3)\n",
      "El recorte del GT (364, 1049, 318) no coincide con el del video (364, 1077, 866, 3)\n",
      "El recorte del GT (244, 1049, 317) no coincide con el del video (244, 1070, 860, 3)\n",
      "El recorte del GT (305, 1048, 317) no coincide con el del video (305, 1076, 865, 3)\n",
      "[Patient 9] 4 pares válidos\n",
      "El recorte del GT (291, 1059, 316) no coincide con el del video (291, 1075, 859, 3)\n",
      "El recorte del GT (331, 1059, 317) no coincide con el del video (331, 1075, 866, 3)\n",
      "El recorte del GT (297, 1058, 317) no coincide con el del video (297, 1074, 866, 3)\n",
      "El recorte del GT (289, 1060, 317) no coincide con el del video (289, 1074, 860, 3)\n",
      "[Patient 12] 4 pares válidos\n",
      "[Patient 13] 4 pares válidos\n",
      "El recorte del GT (301, 1052, 318) no coincide con el del video (301, 1075, 861, 3)\n",
      "El recorte del GT (201, 1017, 318) no coincide con el del video (201, 1017, 860, 3)\n",
      "El recorte del GT (185, 1053, 318) no coincide con el del video (185, 1076, 860, 3)\n",
      "El recorte del GT (271, 1054, 318) no coincide con el del video (271, 1077, 859, 3)\n",
      "[Patient 18] 4 pares válidos\n",
      "[Patient 19] 4 pares válidos\n",
      "[Patient 22] 4 pares válidos\n",
      "[Patient 23] 4 pares válidos\n",
      "[Patient 24] 4 pares válidos\n",
      "[Patient 25] 4 pares válidos\n",
      "[Patient 27] 4 pares válidos\n",
      "El recorte del GT (338, 888, 323) no coincide con el del video (338, 1075, 861, 3)\n",
      "El recorte del GT (355, 876, 323) no coincide con el del video (355, 1038, 860, 3)\n",
      "El recorte del GT (290, 888, 323) no coincide con el del video (290, 1075, 860, 3)\n",
      "El recorte del GT (371, 888, 323) no coincide con el del video (371, 1075, 865, 3)\n",
      "[Patient 32] 4 pares válidos\n",
      "[Patient 33] 4 pares válidos\n",
      "[Patient 35] 4 pares válidos\n",
      "[Patient 37a] 4 pares válidos\n",
      "[Patient 39] 4 pares válidos\n",
      "El recorte del GT (404, 1058, 323) no coincide con el del video (404, 1075, 859, 3)\n",
      "El recorte del GT (330, 1057, 323) no coincide con el del video (330, 1074, 865, 3)\n",
      "El recorte del GT (289, 1050, 324) no coincide con el del video (289, 1067, 866, 3)\n",
      "El recorte del GT (321, 1063, 323) no coincide con el del video (321, 1080, 865, 3)\n",
      "[Patient 41] 4 pares válidos\n",
      "[Patient 42] 4 pares válidos\n",
      "[Patient 49] 4 pares válidos\n",
      "[Patient 52] 4 pares válidos\n",
      "[Patient 54] 4 pares válidos\n",
      "[Patient 58] 4 pares válidos\n",
      "[Patient 59] 4 pares válidos\n",
      "[Patient 61] 4 pares válidos\n",
      "[Patient 62] 4 pares válidos\n",
      "[Patient 63] 4 pares válidos\n",
      "[Patient 64] 4 pares válidos\n",
      "[Patient 66] 4 pares válidos\n",
      "[Patient 67a] 4 pares válidos\n",
      "[Patient 67b] 4 pares válidos\n",
      "[Patient 68] 4 pares válidos\n",
      "[Patient 69] 4 pares válidos\n",
      "[Patient 70] 4 pares válidos\n",
      "[Patient 71] 4 pares válidos\n",
      "[Patient 73] 4 pares válidos\n",
      "[Patient 76] 4 pares válidos\n",
      "[Patient 78a] 4 pares válidos\n",
      "[Patient 78b] 3 pares válidos\n",
      "[Patient 82] 2 pares válidos\n",
      "[Patient 83] 4 pares válidos\n",
      "[Patient 84] 4 pares válidos\n",
      "[Patient 85] 4 pares válidos\n",
      "[Patient 87] 4 pares válidos\n",
      "[Patient 88] 4 pares válidos\n",
      "[Patient 89] 4 pares válidos\n",
      "[Patient 92] 4 pares válidos\n",
      "[Patient 93] 4 pares válidos\n",
      "[Patient 94a] 4 pares válidos\n",
      "[Patient 95] 4 pares válidos\n",
      "[Patient 96] 4 pares válidos\n",
      "[Patient 97] 4 pares válidos\n",
      "El recorte del GT (562, 850, 692) no coincide con el del video (459, 850, 692, 3)\n",
      "[Patient 98] 4 pares válidos\n",
      "[Patient 99] 4 pares válidos\n",
      "[Patient 102] 4 pares válidos\n",
      "[Patient 103] 4 pares válidos\n",
      "[Patient 104] 1 pares válidos\n",
      "[Patient 106] 4 pares válidos\n",
      "[Patient 108] 4 pares válidos\n",
      "[Patient 110] 1 pares válidos\n",
      "[Patient 111] 4 pares válidos\n",
      "[Patient 112] 4 pares válidos\n",
      "[Patient 113] 4 pares válidos\n",
      "[Patient 115a] 4 pares válidos\n",
      "[Patient 117] 4 pares válidos\n",
      "✅ Guardado: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_per_video.csv\n",
      "✅ Guardado con etiquetas: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_per_patient_mean.csv\n",
      "✅ Guardado: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_dataset_mean.csv\n",
      "Total patients processed: 65\n",
      "Total raw patients: 107\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import natsort\n",
    "import numpy as np\n",
    "from torchmetrics import Dice, JaccardIndex, Precision, Recall, Specificity, Accuracy\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import skvideo.io as sio\n",
    "import random\n",
    "import torchio as tio\n",
    "import skimage.morphology as skm\n",
    "from skimage import measure\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from miseval import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "ground_truth_path = '/data/DATA/private_data/Breast studyrc2'\n",
    "videos_path = '/data/DATA/private_data/Breast studyrc2/'\n",
    "\n",
    "\n",
    "# --- regex y helpers ---\n",
    "mat_idx_re_sweep = re.compile(r'_sweep(\\d+)\\.mat$', re.IGNORECASE)\n",
    "mat_idx_re_dash  = re.compile(r'-(\\d+)\\.mat$', re.IGNORECASE)\n",
    "npy_idx_re       = re.compile(r'-(\\d+)_.*masken\\.npy$', re.IGNORECASE)\n",
    "video_idx_re     = re.compile(r'-(\\d+)\\.mp4$', re.IGNORECASE)  # videos: ...-#id.mp4\n",
    "\n",
    "def get_mat_idx(p):\n",
    "    base = os.path.basename(p)\n",
    "    m = mat_idx_re_sweep.search(base)\n",
    "    if m: return int(m.group(1))\n",
    "    m = mat_idx_re_dash.search(base)\n",
    "    if m: return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def get_npy_idx(p):\n",
    "    m = npy_idx_re.search(os.path.basename(p))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def get_video_idx(p):\n",
    "    m = video_idx_re.search(os.path.basename(p))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "#obtain subfolder ids in /data/GitHub/Breast-AI-model/src/predictions_rc2/\n",
    "def get_subfolder_ids(path):\n",
    "    subfolders = glob.glob(os.path.join(path, '*/'))\n",
    "    subfolder_ids = [os.path.basename(os.path.normpath(folder)) for folder in subfolders]\n",
    "    return natsort.natsorted(subfolder_ids)\n",
    "\n",
    "# === Función: obtiene ROI desde el video (igual que tu pipeline) ===\n",
    "def process_video_and_get_crop(video, min_obj_size=1000, hole_size=1000):\n",
    "    video = video[:, :, 0:-50, :]  # elimina borde derecho (artefacto)\n",
    "    D, H, W, _ = video.shape\n",
    "    ref_idx, idx1, idx2 = random.sample(range(D), 3)\n",
    "    diff1 = (video[ref_idx].astype(np.float32) - video[idx1].astype(np.float32)).astype(np.uint8)\n",
    "    diff2 = (video[ref_idx].astype(np.float32) - video[idx2].astype(np.float32)).astype(np.uint8)\n",
    "    gray1 = cv2.cvtColor(diff1, cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(diff2, cv2.COLOR_RGB2GRAY)\n",
    "    mask = np.logical_or(gray1 > 0, gray2 > 0)\n",
    "    mask = skm.binary_erosion(mask)\n",
    "    mask = skm.remove_small_objects(mask, min_size=min_obj_size)\n",
    "    mask = skm.remove_small_holes(mask, area_threshold=hole_size)\n",
    "    mask = skm.opening(mask)\n",
    "    labels = measure.label(mask)\n",
    "    props = measure.regionprops(labels)\n",
    "    if not props:\n",
    "        raise RuntimeError(\"No se encontró ningún objeto tras la limpieza.\")\n",
    "    largest = max(props, key=lambda p: p.area)\n",
    "    minr, minc, maxr, maxc = largest.bbox\n",
    "    cropped_video = video[:, minr:maxr, minc:maxc, :]\n",
    "    return cropped_video, (minr, maxr, minc, maxc)\n",
    "\n",
    "# === Resize de label map con NN a 128³ ===\n",
    "def resize_label_128(label_vol_DHW):\n",
    "    # TorchIO espera (C, D, H, W); usamos C=1\n",
    "    lbl = label_vol_DHW.astype(np.int16)  # o uint8/uint16 según tus clases\n",
    "    img = tio.LabelMap(tensor=lbl[None, ...])  # (1, D, H, W)\n",
    "    img = tio.Resize((128, 128, 128))(img)     # NN por ser LabelMap\n",
    "    out = img.data.numpy()[0]                  # (D, H, W)\n",
    "    return out\n",
    "\n",
    "def resize_video_128(video_DHWC):\n",
    "    # Conversión a escala de grises y resize a (128,128,128)\n",
    "    _, H, W, _ = video_DHWC.shape\n",
    "    # gray_vol = np.zeros((1, D, H, W), dtype=np.uint8)\n",
    "    # for i in range(D):\n",
    "    #     gray = np.dot(video[i], [0.2989, 0.5870, 0.1140])\n",
    "    #     gray_vol[0, i] = gray.astype(np.uint8)\n",
    "    # coefs float32 para no caer en float64\n",
    "    coefs = np.array([0.2989, 0.5870, 0.1140], dtype=np.float32)\n",
    "    # → esto te da float32\n",
    "    gray = np.tensordot(video_DHWC, coefs, axes=([3], [0]))\n",
    "\n",
    "    # normaliza en [0,1] y SIGUE siendo float32\n",
    "    gray = gray / np.float32(255.0)\n",
    "\n",
    "    # 3. Construye el ScalarImage YA en float32 normalizado\n",
    "    img = tio.ScalarImage(tensor=gray[None, ...])  # shape (1,D,H,W), dtype float32\n",
    "    #solo cambiar tamaño temporal a 128, manteniendo H W sin cambiar\n",
    "    img = tio.Resize((128, H, W))(img)\n",
    "    out = img.data.numpy()  # shape (1,128,128,128), dtype float32\n",
    "    return out\n",
    "\n",
    "def gt_to_128_from_video(gt_DHW, video_path, roi_coords=None, trim_right_px=50):\n",
    "    \"\"\"\n",
    "    gt_DHW: volumen GT (D,H,W)\n",
    "    video_path: mp4 correspondiente (mismo índice)\n",
    "    roi_coords: (minr, maxr, minc, maxc) si ya lo tienes; si no, se calcula del video\n",
    "    \"\"\"\n",
    "    crop_flag=False\n",
    "    # 1) Cargar video y (si no hay coords) calcular ROI exactamente como en inferencia\n",
    "    video = sio.vread(video_path)  # (D,H,W,3)\n",
    "    if roi_coords is None:\n",
    "        cropped_video, roi_coords = process_video_and_get_crop(video)\n",
    "    minr, maxr, minc, maxc = roi_coords\n",
    "\n",
    "    # 2) Aplicar recorte de borde derecho también al GT\n",
    "    if trim_right_px and gt_DHW.shape[2] > trim_right_px:\n",
    "        gt_DHW = gt_DHW[:, :, 0:-trim_right_px]\n",
    "\n",
    "    # 3) Recortar GT con el mismo ROI del video\n",
    "    gt_crop = gt_DHW[:, minr:maxr, minc:maxc]\n",
    "    if gt_crop.size != cropped_video[:,:,:,0].size:\n",
    "        crop_flag = True\n",
    "        print(f\"El recorte del GT {gt_crop.shape} no coincide con el del video {cropped_video.shape}\")\n",
    "        \n",
    "    # 4) Resize NN a 128³\n",
    "    gt_128 = resize_label_128(gt_crop)\n",
    "    cropped_video = resize_video_128(cropped_video)[0]  # (128, H', W', 3)\n",
    "    return gt_128, roi_coords,cropped_video,crop_flag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dir = \"/data/GitHub/Breast-AI-model/src/metrics_rc2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "metric_list = [\"DSC\", \"IoU\", \"AHD\", \"VS\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"AUC\",\"Kap\"]\n",
    "\n",
    "def safe_eval(gt, pred, metric):\n",
    "    try:\n",
    "        return evaluate(gt, pred, metric=metric)\n",
    "    except Exception as e:\n",
    "        # opcional: print(f\"[WARN] {metric} falló: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Acumuladores\n",
    "per_video_rows = []\n",
    "\n",
    "subfolders_ids = natsort.natsorted(get_subfolder_ids('/data/GitHub/Breast-AI-model/src/predictions_rc2/'))\n",
    "count_patients = 0\n",
    "\n",
    "for subfolder_id in subfolders_ids:\n",
    "    ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'FINAL LABELS')\n",
    "    if not os.path.exists(ground_truth_folder):\n",
    "        ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'Segmentations')\n",
    "        if not os.path.exists(ground_truth_folder):\n",
    "            continue\n",
    "    count_patients += 1\n",
    "\n",
    "    ground_truth_files = glob.glob(os.path.join(ground_truth_folder, '**', '*.mat'), recursive=True)\n",
    "    ground_truth_files = natsort.natsorted([f for f in ground_truth_files\n",
    "                                            if 'groundTruthMed.mat' not in f and 'Session.mat' not in f])\n",
    "\n",
    "    pred_files = glob.glob(os.path.join('/data/GitHub/Breast-AI-model/src/predictions_rc2', subfolder_id, '*.npy'))\n",
    "    pred_files = natsort.natsorted(pred_files)\n",
    "\n",
    "    patterns = [\n",
    "    os.path.join(videos_path, subfolder_id, 'VSI 8 [cC]lips', '*.[mM][pP]4'),\n",
    "    os.path.join(videos_path, subfolder_id, '8 VSI [cC]lips', '*.[mM][pP]4'),  # opcional, por si existe esa variante\n",
    "]\n",
    "\n",
    "    videos_files = []\n",
    "    for pat in patterns:\n",
    "        videos_files += glob.glob(pat)\n",
    "\n",
    "    # quitar duplicados y ordenar de forma natural\n",
    "    videos_files = natsort.natsorted({os.path.realpath(p) for p in videos_files})\n",
    "    # idx -> path\n",
    "    npy_by_idx = {get_npy_idx(p): p for p in pred_files if get_npy_idx(p) is not None}\n",
    "    videos_by_idx = {get_video_idx(v): v for v in videos_files if get_video_idx(v) is not None}\n",
    "\n",
    "    # (gt_mat, pred_npy, video_mp4, idx)\n",
    "    pairs = []\n",
    "    for mp in ground_truth_files:\n",
    "        i = get_mat_idx(mp)\n",
    "        if i is None:\n",
    "            continue\n",
    "        pred_p = npy_by_idx.get(i)\n",
    "        vid_p  = videos_by_idx.get(i)\n",
    "        if pred_p is not None and vid_p is not None:   # si no hay video, omitimos porque el ROI depende del video\n",
    "            pairs.append((mp, pred_p, vid_p, i))\n",
    "\n",
    "    print(f\"[{subfolder_id}] {len(pairs)} pares válidos\")\n",
    "\n",
    "    # ---- Métricas por video ----\n",
    "    for gt_file, pred_file, vid_file, idx in pairs:\n",
    "        try:\n",
    "            # carga GT, alinea y redimensiona\n",
    "            gt = np.transpose(loadmat(gt_file)['labels'], (2, 0, 1))  # (D,H,W)\n",
    "            gt_128, coords, cropped_video,crop_flag = gt_to_128_from_video(gt, vid_file)\n",
    "\n",
    "            # carga pred\n",
    "            pred = np.load(pred_file)\n",
    "\n",
    "            # binariza por seguridad\n",
    "            gt_bin   = (gt_128 > 0).astype(np.uint8)\n",
    "            pred_bin = (pred   > 0).astype(np.uint8)\n",
    "\n",
    "            # evalúa todas las métricas (con tolerancia a fallos)\n",
    "            row = {\n",
    "                \"subject_id\": subfolder_id.split(' ')[-1],\n",
    "                \"sweep\": idx,\n",
    "                \"D\": gt_bin.shape[0], \"H\": gt_bin.shape[1], \"W\": gt_bin.shape[2],\n",
    "                'Crop flag': crop_flag\n",
    "            }\n",
    "            for m in metric_list:\n",
    "                row[m] = np.round(safe_eval(gt_bin, pred_bin, metric=m),2)\n",
    "\n",
    "            per_video_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Loguea y sigue con el siguiente video\n",
    "            print(f\"[WARN] Falló subject={subfolder_id} idx={idx}: {e}\")\n",
    "            continue\n",
    "# ---- DataFrames y CSVs ----\n",
    "df_videos = pd.DataFrame(per_video_rows)\n",
    "\n",
    "# CSV 1: métricas individuales por video\n",
    "csv_per_video = os.path.join(output_dir, \"metrics_per_video.csv\")\n",
    "df_videos.to_csv(csv_per_video, index=False)\n",
    "print(f\"✅ Guardado: {csv_per_video}\")\n",
    "\n",
    "# === NORMALIZAR IDs EN df_videos (crea un ID canónico numérico) ===\n",
    "# subject_id típicamente es \"Patient 2\", \"Patient 03\", etc. Extraemos el entero.\n",
    "df_videos['patient_num'] = (\n",
    "    df_videos['subject_id'].astype(str).str.extract(r'(\\d+)')[0].astype('Int64')\n",
    ")\n",
    "\n",
    "# CSV 2: promedios por paciente (ignora NaN) + Crop flag\n",
    "agg_cols = metric_list  # métricas a promediar\n",
    "agg_dict = {m: 'mean' for m in agg_cols}\n",
    "agg_dict['Crop flag'] = 'max'   # True si algún video del paciente tuvo Crop flag\n",
    "\n",
    "df_patients = (\n",
    "    df_videos\n",
    "    .groupby(['subject_id', 'patient_num'], dropna=False)\n",
    "    .agg(agg_dict)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ==== MERGE CON BS.csv (usa patient_num) ====\n",
    "bs_csv = '/data/GitHub/Breast-AI-model/src/BS.csv'  # ajusta si es necesario\n",
    "bs = pd.read_csv(bs_csv)\n",
    "\n",
    "# Normaliza nombres y tipos\n",
    "bs.columns = [c.strip() for c in bs.columns]\n",
    "esperadas = ['ID', 'Mass/no mass', 'Cancer/no cancer', 'Category']\n",
    "faltantes = [c for c in esperadas if c not in bs.columns]\n",
    "if faltantes:\n",
    "    raise ValueError(f\"Faltan columnas en BS.csv: {faltantes}\")\n",
    "\n",
    "# Crea también el ID canónico en BS.csv\n",
    "bs['ID_str'] = bs['ID'].astype(str).str.strip()\n",
    "bs['patient_num'] = bs['ID_str'].str.extract(r'(\\d+)')[0].astype('Int64')\n",
    "bs = bs.drop_duplicates(subset='patient_num', keep='first')\n",
    "\n",
    "# Merge por patient_num\n",
    "df_patients_labeled = (\n",
    "    df_patients.merge(\n",
    "        bs[['patient_num','Mass/no mass','Cancer/no cancer','Category']],\n",
    "        on='patient_num', how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# (Opcional) mover columnas de etiqueta al inicio\n",
    "label_cols = ['Mass/no mass','Cancer/no cancer','Category']\n",
    "ordered_cols = (['subject_id','patient_num','Crop flag'] + label_cols +\n",
    "                [c for c in df_patients_labeled.columns if c not in (['subject_id','patient_num','Crop flag'] + label_cols)])\n",
    "df_patients_labeled = df_patients_labeled[ordered_cols]\n",
    "\n",
    "# Reporta pacientes sin match\n",
    "sin_match = df_patients_labeled[df_patients_labeled['Mass/no mass'].isna()]['subject_id'].tolist()\n",
    "if sin_match:\n",
    "    print(f\"[INFO] {len(sin_match)} pacientes sin match en BS.csv (ejemplos): {sin_match[:10]}\")\n",
    "\n",
    "\n",
    "csv_per_patient_labeled = os.path.join(output_dir, \"metrics_per_patient_mean.csv\")\n",
    "df_patients_labeled.set_index('subject_id').drop(columns=['patient_num']).to_csv(csv_per_patient_labeled)\n",
    "print(f\"✅ Guardado con etiquetas: {csv_per_patient_labeled}\")\n",
    "\n",
    "# CSV 3: promedio global del dataset (ignora NaN)\n",
    "global_means = df_videos[agg_cols].mean(numeric_only=True)\n",
    "df_global = pd.DataFrame(global_means).T\n",
    "df_global.index = [\"dataset_mean\"]\n",
    "csv_global = os.path.join(output_dir, \"metrics_dataset_mean.csv\")\n",
    "df_global.to_csv(csv_global)\n",
    "print(f\"✅ Guardado: {csv_global}\")\n",
    "\n",
    "\n",
    "        #Mostrar el primer frame no vacío del gt_128\n",
    "        #first_non_empty_frame = np.argmax(np.any(gt_128, axis=(1, 2)))\n",
    "        #mostrar el video, el ground_truth y la predicción\n",
    "        # if first_non_empty_frame < gt_128.shape[0]:\n",
    "        #     plt.figure(figsize=(12, 4))\n",
    "        #     plt.subplot(1, 3, 1)\n",
    "        #     plt.imshow(gt_128[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title('Ground Truth Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(1, 3, 2)\n",
    "        #     plt.imshow(pred[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title('Prediction Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(1, 3, 3)\n",
    "        #     plt.imshow(cropped_video[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title  # 2) Aplicar recorte de borde derecho también al GT\n",
    "        #     ('Video Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.show()\n",
    "        #print(\"Shapes -> GT:\", ground_truth.shape, \"| Pred:\", prediction.shape, \"| Video:\", video.shape)\n",
    "  \n",
    "\n",
    "    #break\n",
    "print(f\"Total patients processed: {count_patients}\")\n",
    "print(f\"Total raw patients: {len(subfolders_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bd7a3",
   "metadata": {},
   "source": [
    "# Generacion de OVERLAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dbfa412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Patient 2] 4 pares válidos\n",
      "(273, 1080, 1854, 3) (273, 1076, 866, 3) (273, 1080, 1804) (273, 1076, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 1_overlay.mp4\n",
      "(361, 1080, 1854, 3) (361, 1075, 866, 3) (361, 1080, 1804) (361, 1075, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 4_overlay.mp4\n",
      "(301, 1080, 1854, 3) (301, 1077, 860, 3) (301, 1080, 1804) (301, 1077, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 5_overlay.mp4\n",
      "(332, 1080, 1854, 3) (332, 1075, 862, 3) (332, 1080, 1804) (332, 1075, 862)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 8_overlay.mp4\n",
      "[Patient 3] 4 pares válidos\n",
      "(393, 1080, 1854, 3) (393, 1077, 861, 3) (393, 1052, 811) (393, 1049, 318)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=2: \n",
      "(364, 1080, 1854, 3) (364, 1064, 859, 3) (364, 1052, 811) (364, 1036, 317)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=4: \n",
      "(244, 1080, 1854, 3) (244, 1073, 862, 3) (244, 1052, 811) (244, 1047, 319)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=6: \n",
      "(305, 1080, 1854, 3) (305, 1072, 866, 3) (305, 1052, 811) (305, 1048, 318)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=8: \n",
      "[Patient 9] 4 pares válidos\n",
      "(291, 1080, 1854, 3) (291, 1075, 861, 3) (291, 1064, 810) (291, 1059, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=1: \n",
      "(331, 1080, 1854, 3) (331, 1075, 864, 3) (331, 1064, 810) (331, 1059, 318)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=4: \n",
      "(297, 1080, 1854, 3) (297, 1075, 867, 3) (297, 1064, 810) (297, 1059, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=5: \n",
      "(289, 1080, 1854, 3) (289, 1067, 861, 3) (289, 1064, 810) (289, 1051, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=8: \n",
      "[Patient 12] 4 pares válidos\n",
      "(363, 1080, 1854, 3) (363, 1073, 859, 3) (363, 1080, 1804) (363, 1073, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 1_overlay.mp4\n",
      "(306, 1080, 1854, 3) (306, 1077, 859, 3) (306, 1080, 1804) (306, 1077, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 4_overlay.mp4\n",
      "(265, 1080, 1854, 3) (265, 1075, 857, 3) (265, 1080, 1804) (265, 1075, 857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (265, 1075, 858) != target shape (265, 1075, 857). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 6_overlay.mp4\n",
      "(239, 1080, 1854, 3) (239, 842, 859, 3) (239, 1080, 1804) (239, 842, 859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 842, 859) != target shape (239, 842, 859). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 8_overlay.mp4\n",
      "[Patient 13] 4 pares válidos\n",
      "(301, 1080, 1854, 3) (301, 1067, 862, 3) (301, 1057, 812) (301, 1044, 319)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=1: \n",
      "(201, 1080, 1854, 3) (201, 1033, 861, 3) (201, 1057, 812) (201, 1033, 319)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=4: \n",
      "(185, 1080, 1854, 3) (185, 1075, 860, 3) (185, 1057, 812) (185, 1054, 318)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=6: \n",
      "(271, 1080, 1854, 3) (271, 1075, 860, 3) (271, 1057, 812) (271, 1054, 318)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=8: \n",
      "[Patient 18] 4 pares válidos\n",
      "(249, 1080, 1854, 3) (249, 1064, 832, 3) (249, 1080, 1804) (249, 1064, 832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 1064, 832) != target shape (249, 1064, 832). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 2_overlay.mp4\n",
      "(275, 1080, 1854, 3) (275, 488, 804, 3) (275, 1080, 1804) (275, 488, 804)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 4_overlay.mp4\n",
      "(293, 1080, 1854, 3) (293, 1077, 860, 3) (293, 1080, 1804) (293, 1077, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 5_overlay.mp4\n",
      "(232, 1080, 1854, 3) (232, 1076, 866, 3) (232, 1080, 1804) (232, 1076, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 7_overlay.mp4\n",
      "[Patient 19] 4 pares válidos\n",
      "(399, 1080, 1854, 3) (399, 1072, 860, 3) (399, 1080, 1804) (399, 1072, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 2_overlay.mp4\n",
      "(354, 1080, 1854, 3) (354, 1073, 860, 3) (354, 1080, 1804) (354, 1073, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 4_overlay.mp4\n",
      "(403, 1080, 1854, 3) (403, 1077, 866, 3) (403, 1080, 1804) (403, 1077, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 5_overlay.mp4\n",
      "(342, 1080, 1854, 3) (342, 1077, 865, 3) (342, 1080, 1804) (342, 1077, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 8_overlay.mp4\n",
      "[Patient 22] 4 pares válidos\n",
      "(431, 1080, 1854, 3) (431, 1058, 865, 3) (431, 1080, 1804) (431, 1058, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 1_overlay.mp4\n",
      "(447, 1080, 1854, 3) (447, 1072, 865, 3) (447, 1080, 1804) (447, 1072, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 4_overlay.mp4\n",
      "(477, 1080, 1854, 3) (477, 1071, 860, 3) (477, 1080, 1804) (477, 1071, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 6_overlay.mp4\n",
      "(443, 1080, 1854, 3) (443, 903, 864, 3) (443, 1080, 1804) (443, 903, 864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (444, 903, 864) != target shape (443, 903, 864). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 7_overlay.mp4\n",
      "[Patient 23] 4 pares válidos\n",
      "(572, 1080, 1854, 3) (572, 587, 834, 3) (572, 1080, 1804) (572, 587, 834)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (572, 587, 835) != target shape (572, 587, 834). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 2_overlay.mp4\n",
      "(429, 1080, 1854, 3) (429, 648, 865, 3) (429, 1080, 1804) (429, 648, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 4_overlay.mp4\n",
      "(780, 1080, 1854, 3) (780, 573, 837, 3) (780, 1080, 1804) (780, 573, 837)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 5_overlay.mp4\n",
      "(395, 1080, 1854, 3) (395, 789, 857, 3) (395, 1080, 1804) (395, 789, 857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (395, 789, 858) != target shape (395, 789, 857). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 7_overlay.mp4\n",
      "[Patient 24] 4 pares válidos\n",
      "(603, 1080, 1854, 3) (603, 1067, 860, 3) (603, 1080, 1804) (603, 1067, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 1_overlay.mp4\n",
      "(426, 1080, 1854, 3) (426, 1075, 860, 3) (426, 1080, 1804) (426, 1075, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 3_overlay.mp4\n",
      "(369, 1080, 1854, 3) (369, 1076, 859, 3) (369, 1080, 1804) (369, 1076, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 5_overlay.mp4\n",
      "(355, 1080, 1854, 3) (355, 1075, 866, 3) (355, 1080, 1804) (355, 1075, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 7_overlay.mp4\n",
      "[Patient 25] 4 pares válidos\n",
      "(569, 1080, 1854, 3) (569, 1075, 860, 3) (569, 1080, 1804) (569, 1075, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 1_overlay.mp4\n",
      "(474, 1080, 1854, 3) (474, 1073, 861, 3) (474, 1080, 1804) (474, 1073, 861)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (475, 1073, 861) != target shape (474, 1073, 861). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 3_overlay.mp4\n",
      "(293, 1080, 1854, 3) (293, 1067, 865, 3) (293, 1080, 1804) (293, 1067, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 6_overlay.mp4\n",
      "(416, 1080, 1854, 3) (416, 1060, 866, 3) (416, 1080, 1804) (416, 1060, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 8_overlay.mp4\n",
      "[Patient 27] 4 pares válidos\n",
      "(338, 1080, 1854, 3) (338, 1064, 865, 3) (338, 893, 817) (338, 877, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=2: \n",
      "(355, 1080, 1854, 3) (355, 1070, 865, 3) (355, 893, 817) (355, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=3: \n",
      "(290, 1080, 1854, 3) (290, 1075, 859, 3) (290, 893, 817) (290, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=6: \n",
      "(371, 1080, 1854, 3) (371, 1075, 860, 3) (371, 893, 817) (371, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=7: \n",
      "[Patient 32] 4 pares válidos\n",
      "(414, 1080, 1854, 3) (414, 1076, 860, 3) (414, 1080, 1804) (414, 1076, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 2_overlay.mp4\n",
      "(369, 1080, 1854, 3) (369, 1076, 865, 3) (369, 1080, 1804) (369, 1076, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 4_overlay.mp4\n",
      "(221, 1080, 1854, 3) (221, 1064, 865, 3) (221, 1080, 1804) (221, 1064, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 6_overlay.mp4\n",
      "(385, 1080, 1854, 3) (385, 1068, 865, 3) (385, 1080, 1804) (385, 1068, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 7_overlay.mp4\n",
      "[Patient 33] 4 pares válidos\n",
      "(320, 1080, 1854, 3) (320, 973, 865, 3) (320, 1080, 1804) (320, 973, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 1_overlay.mp4\n",
      "(249, 1080, 1854, 3) (249, 1064, 860, 3) (249, 1080, 1804) (249, 1064, 860)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 1064, 860) != target shape (249, 1064, 860). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 4_overlay.mp4\n",
      "(220, 1080, 1854, 3) (220, 1070, 866, 3) (220, 1080, 1804) (220, 1070, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 5_overlay.mp4\n",
      "(246, 1080, 1854, 3) (246, 1075, 859, 3) (246, 1080, 1804) (246, 1075, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 7_overlay.mp4\n",
      "[Patient 35] 4 pares válidos\n",
      "(378, 1080, 1854, 3) (378, 1075, 865, 3) (378, 1080, 1804) (378, 1075, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 1_overlay.mp4\n",
      "(241, 1080, 1854, 3) (241, 1073, 860, 3) (241, 1080, 1804) (241, 1073, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 4_overlay.mp4\n",
      "(175, 1080, 1854, 3) (175, 1064, 866, 3) (175, 1080, 1804) (175, 1064, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 6_overlay.mp4\n",
      "(303, 1080, 1854, 3) (303, 1076, 861, 3) (303, 1080, 1804) (303, 1076, 861)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 8_overlay.mp4\n",
      "[Patient 37a] 4 pares válidos\n",
      "(360, 1080, 1854, 3) (360, 1076, 867, 3) (360, 1080, 1804) (360, 1076, 867)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 1_overlay.mp4\n",
      "(311, 1080, 1854, 3) (311, 1071, 865, 3) (311, 1080, 1804) (311, 1071, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 4_overlay.mp4\n",
      "(302, 1080, 1854, 3) (302, 1075, 865, 3) (302, 1080, 1804) (302, 1075, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 6_overlay.mp4\n",
      "(353, 1080, 1854, 3) (353, 1066, 861, 3) (353, 1080, 1804) (353, 1066, 861)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 8_overlay.mp4\n",
      "[Patient 39] 4 pares válidos\n",
      "(404, 1080, 1854, 3) (404, 1080, 865, 3) (404, 1063, 817) (404, 1063, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=2: \n",
      "(330, 1080, 1854, 3) (330, 1076, 866, 3) (330, 1063, 817) (330, 1059, 324)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=4: \n",
      "(289, 1080, 1854, 3) (289, 1076, 865, 3) (289, 1063, 817) (289, 1059, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=5: \n",
      "(321, 1080, 1854, 3) (321, 1073, 865, 3) (321, 1063, 817) (321, 1059, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=8: \n",
      "[Patient 41] 4 pares válidos\n",
      "(403, 1080, 1854, 3) (403, 1076, 859, 3) (403, 1080, 1804) (403, 1076, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 2_overlay.mp4\n",
      "(429, 1080, 1854, 3) (429, 1076, 859, 3) (429, 1080, 1804) (429, 1076, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 4_overlay.mp4\n",
      "(306, 1080, 1854, 3) (306, 1077, 865, 3) (306, 1080, 1804) (306, 1077, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 5_overlay.mp4\n",
      "(349, 1080, 1854, 3) (349, 1072, 865, 3) (349, 1080, 1804) (349, 1072, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 8_overlay.mp4\n",
      "[Patient 42] 4 pares válidos\n",
      "(431, 1080, 854, 3) (431, 838, 691, 3) (431, 1080, 804) (431, 838, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 2_overlay.mp4\n",
      "(337, 1080, 854, 3) (337, 842, 689, 3) (337, 1080, 804) (337, 842, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 3_overlay.mp4\n",
      "(309, 1080, 854, 3) (309, 842, 690, 3) (309, 1080, 804) (309, 842, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 5_overlay.mp4\n",
      "(248, 1080, 854, 3) (248, 843, 691, 3) (248, 1080, 804) (248, 843, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 7_overlay.mp4\n",
      "[Patient 49] 4 pares válidos\n",
      "(196, 1080, 1854, 3) (196, 1075, 863, 3) (196, 1080, 1804) (196, 1075, 863)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (197, 1075, 863) != target shape (196, 1075, 863). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 2_overlay.mp4\n",
      "(258, 1080, 1854, 3) (258, 1075, 865, 3) (258, 1080, 1804) (258, 1075, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 4_overlay.mp4\n",
      "(270, 1080, 1854, 3) (270, 1064, 865, 3) (270, 1080, 1804) (270, 1064, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 5_overlay.mp4\n",
      "(192, 1080, 1854, 3) (192, 1065, 865, 3) (192, 1080, 1804) (192, 1065, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 8_overlay.mp4\n",
      "[Patient 52] 4 pares válidos\n",
      "(383, 1080, 1854, 3) (383, 1075, 866, 3) (383, 1080, 1804) (383, 1075, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 1_overlay.mp4\n",
      "(370, 1080, 1854, 3) (370, 1076, 860, 3) (370, 1080, 1804) (370, 1076, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 4_overlay.mp4\n",
      "(264, 1080, 1854, 3) (264, 1067, 864, 3) (264, 1080, 1804) (264, 1067, 864)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 6_overlay.mp4\n",
      "(438, 1080, 1854, 3) (438, 1064, 866, 3) (438, 1080, 1804) (438, 1064, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 8_overlay.mp4\n",
      "[Patient 54] 4 pares válidos\n",
      "(409, 1080, 1854, 3) (409, 1074, 860, 3) (409, 1080, 1804) (409, 1074, 860)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 1_overlay.mp4\n",
      "(337, 1080, 1854, 3) (337, 1077, 862, 3) (337, 1080, 1804) (337, 1077, 862)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 4_overlay.mp4\n",
      "(281, 1080, 1854, 3) (281, 1076, 862, 3) (281, 1080, 1804) (281, 1076, 862)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 6_overlay.mp4\n",
      "(330, 1080, 1854, 3) (330, 1075, 861, 3) (330, 1080, 1804) (330, 1075, 861)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 7_overlay.mp4\n",
      "[Patient 58] 4 pares válidos\n",
      "(292, 1080, 1854, 3) (292, 426, 822, 3) (292, 1080, 1804) (292, 426, 822)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 2_overlay.mp4\n",
      "(280, 1080, 1854, 3) (280, 1015, 859, 3) (280, 1080, 1804) (280, 1015, 859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (280, 1016, 859) != target shape (280, 1015, 859). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 3_overlay.mp4\n",
      "(251, 1080, 1854, 3) (251, 787, 854, 3) (251, 1080, 1804) (251, 787, 854)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 6_overlay.mp4\n",
      "(526, 1080, 1854, 3) (526, 734, 849, 3) (526, 1080, 1804) (526, 734, 849)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (526, 734, 850) != target shape (526, 734, 849). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 8_overlay.mp4\n",
      "[Patient 59] 4 pares válidos\n",
      "(280, 1080, 1854, 3) (280, 1066, 866, 3) (280, 1080, 1804) (280, 1066, 866)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 2_overlay.mp4\n",
      "(280, 1080, 1854, 3) (280, 1076, 865, 3) (280, 1080, 1804) (280, 1076, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 4_overlay.mp4\n",
      "(300, 1080, 1854, 3) (300, 1075, 865, 3) (300, 1080, 1804) (300, 1075, 865)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 6_overlay.mp4\n",
      "(244, 1080, 1854, 3) (244, 1065, 859, 3) (244, 1080, 1804) (244, 1065, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 8_overlay.mp4\n",
      "[Patient 61] 4 pares válidos\n",
      "(295, 1080, 1854, 3) (295, 1076, 859, 3) (295, 1080, 1804) (295, 1076, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 2_overlay.mp4\n",
      "(300, 1080, 1854, 3) (300, 1077, 859, 3) (300, 1080, 1804) (300, 1077, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 3_overlay.mp4\n",
      "(210, 1080, 1854, 3) (210, 316, 758, 3) (210, 1080, 1804) (210, 316, 758)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 5_overlay.mp4\n",
      "(213, 1080, 1854, 3) (213, 680, 807, 3) (213, 1080, 1804) (213, 680, 807)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 8_overlay.mp4\n",
      "[Patient 62] 4 pares válidos\n",
      "(379, 1080, 1854, 3) (379, 1077, 858, 3) (379, 1080, 1804) (379, 1077, 858)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 1_overlay.mp4\n",
      "(281, 1080, 1854, 3) (281, 891, 861, 3) (281, 1080, 1804) (281, 891, 861)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 4_overlay.mp4\n",
      "(205, 1080, 1854, 3) (205, 799, 859, 3) (205, 1080, 1804) (205, 799, 859)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 6_overlay.mp4\n",
      "(355, 1080, 1854, 3) (355, 207, 698, 3) (355, 1080, 1804) (355, 207, 698)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 8_overlay.mp4\n",
      "[Patient 63] 4 pares válidos\n",
      "(463, 1080, 854, 3) (463, 708, 690, 3) (463, 1080, 804) (463, 708, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 1_overlay.mp4\n",
      "(458, 1080, 854, 3) (458, 365, 654, 3) (458, 1080, 804) (458, 365, 654)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 4_overlay.mp4\n",
      "(598, 1080, 854, 3) (598, 390, 646, 3) (598, 1080, 804) (598, 390, 646)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 5_overlay.mp4\n",
      "(485, 1080, 854, 3) (485, 432, 661, 3) (485, 1080, 804) (485, 432, 661)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 8_overlay.mp4\n",
      "[Patient 64] 4 pares válidos\n",
      "(562, 1080, 854, 3) (562, 834, 690, 3) (562, 1080, 804) (562, 834, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (562, 835, 690) != target shape (562, 834, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 1_overlay.mp4\n",
      "(563, 1080, 854, 3) (563, 544, 678, 3) (563, 1080, 804) (563, 544, 678)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 4_overlay.mp4\n",
      "(501, 1080, 854, 3) (501, 746, 689, 3) (501, 1080, 804) (501, 746, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (502, 746, 689) != target shape (501, 746, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 5_overlay.mp4\n",
      "(661, 1080, 854, 3) (661, 696, 688, 3) (661, 1080, 804) (661, 696, 688)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 8_overlay.mp4\n",
      "[Patient 66] 4 pares válidos\n",
      "(459, 1080, 854, 3) (459, 846, 689, 3) (459, 1080, 804) (459, 846, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 1_overlay.mp4\n",
      "(477, 1080, 854, 3) (477, 844, 690, 3) (477, 1080, 804) (477, 844, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 3_overlay.mp4\n",
      "(422, 1080, 854, 3) (422, 849, 690, 3) (422, 1080, 804) (422, 849, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (422, 850, 690) != target shape (422, 849, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 5_overlay.mp4\n",
      "(485, 1080, 854, 3) (485, 850, 690, 3) (485, 1080, 804) (485, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (485, 851, 690) != target shape (485, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 8_overlay.mp4\n",
      "[Patient 67a] 4 pares válidos\n",
      "(404, 1080, 854, 3) (404, 844, 690, 3) (404, 1080, 804) (404, 844, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 2_overlay.mp4\n",
      "(345, 1080, 854, 3) (345, 842, 690, 3) (345, 1080, 804) (345, 842, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 4_overlay.mp4\n",
      "(405, 1080, 854, 3) (405, 844, 690, 3) (405, 1080, 804) (405, 844, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 6_overlay.mp4\n",
      "(430, 1080, 854, 3) (430, 844, 689, 3) (430, 1080, 804) (430, 844, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 7_overlay.mp4\n",
      "[Patient 67b] 4 pares válidos\n",
      "(337, 1080, 854, 3) (337, 844, 690, 3) (337, 1080, 804) (337, 844, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 2_overlay.mp4\n",
      "(392, 1080, 854, 3) (392, 842, 690, 3) (392, 1080, 804) (392, 842, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (393, 842, 690) != target shape (392, 842, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 3_overlay.mp4\n",
      "(373, 1080, 854, 3) (373, 843, 690, 3) (373, 1080, 804) (373, 843, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 5_overlay.mp4\n",
      "(406, 1080, 854, 3) (406, 842, 691, 3) (406, 1080, 804) (406, 842, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 8_overlay.mp4\n",
      "[Patient 68] 4 pares válidos\n",
      "(479, 1080, 854, 3) (479, 844, 689, 3) (479, 1080, 804) (479, 844, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (480, 844, 689) != target shape (479, 844, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 1_overlay.mp4\n",
      "(547, 1080, 854, 3) (547, 850, 690, 3) (547, 1080, 804) (547, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (547, 851, 690) != target shape (547, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 3_overlay.mp4\n",
      "(442, 1080, 854, 3) (442, 849, 689, 3) (442, 1080, 804) (442, 849, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (442, 850, 689) != target shape (442, 849, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 5_overlay.mp4\n",
      "(439, 1080, 854, 3) (439, 849, 690, 3) (439, 1080, 804) (439, 849, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (439, 850, 690) != target shape (439, 849, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 7_overlay.mp4\n",
      "[Patient 69] 4 pares válidos\n",
      "(246, 1080, 854, 3) (246, 850, 690, 3) (246, 1080, 804) (246, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (246, 851, 690) != target shape (246, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 2_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 843, 690, 3) (242, 1080, 804) (242, 843, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 3_overlay.mp4\n",
      "(240, 1080, 854, 3) (240, 845, 691, 3) (240, 1080, 804) (240, 845, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 6_overlay.mp4\n",
      "(236, 1080, 854, 3) (236, 844, 691, 3) (236, 1080, 804) (236, 844, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 8_overlay.mp4\n",
      "[Patient 70] 4 pares válidos\n",
      "(421, 1080, 858, 3) (421, 855, 771, 3) (421, 1080, 808) (421, 855, 771)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 2_overlay.mp4\n",
      "(355, 1080, 858, 3) (355, 855, 692, 3) (355, 1080, 808) (355, 855, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 3_overlay.mp4\n",
      "(432, 1080, 858, 3) (432, 856, 728, 3) (432, 1080, 808) (432, 856, 728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (432, 857, 728) != target shape (432, 856, 728). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 6_overlay.mp4\n",
      "(544, 1080, 858, 3) (544, 857, 808, 3) (544, 1080, 808) (544, 857, 808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (544, 858, 808) != target shape (544, 857, 808). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 7_overlay.mp4\n",
      "[Patient 71] 4 pares válidos\n",
      "(464, 1080, 858, 3) (464, 854, 697, 3) (464, 1080, 808) (464, 854, 697)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 2_overlay.mp4\n",
      "(539, 1080, 858, 3) (539, 851, 692, 3) (539, 1080, 808) (539, 851, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 3_overlay.mp4\n",
      "(672, 1080, 858, 3) (672, 855, 694, 3) (672, 1080, 808) (672, 855, 694)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 5_overlay.mp4\n",
      "(605, 1080, 858, 3) (605, 853, 693, 3) (605, 1080, 808) (605, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (605, 854, 693) != target shape (605, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 7_overlay.mp4\n",
      "[Patient 73] 4 pares válidos\n",
      "(427, 1080, 854, 3) (427, 842, 689, 3) (427, 1080, 804) (427, 842, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 1_overlay.mp4\n",
      "(600, 1080, 854, 3) (600, 832, 687, 3) (600, 1080, 804) (600, 832, 687)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 4_overlay.mp4\n",
      "(576, 1080, 854, 3) (576, 658, 687, 3) (576, 1080, 804) (576, 658, 687)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 6_overlay.mp4\n",
      "(448, 1080, 854, 3) (448, 834, 688, 3) (448, 1080, 804) (448, 834, 688)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (448, 835, 688) != target shape (448, 834, 688). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 8_overlay.mp4\n",
      "[Patient 76] 4 pares válidos\n",
      "(217, 1080, 854, 3) (217, 840, 691, 3) (217, 1080, 804) (217, 840, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 1_overlay.mp4\n",
      "(244, 1080, 854, 3) (244, 844, 689, 3) (244, 1080, 804) (244, 844, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 4_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 844, 689, 3) (242, 1080, 804) (242, 844, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 5_overlay.mp4\n",
      "(240, 1080, 854, 3) (240, 843, 690, 3) (240, 1080, 804) (240, 843, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 7_overlay.mp4\n",
      "[Patient 78a] 4 pares válidos\n",
      "(466, 1080, 858, 3) (466, 849, 692, 3) (466, 1080, 808) (466, 849, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (466, 850, 692) != target shape (466, 849, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 1_overlay.mp4\n",
      "(553, 1080, 858, 3) (553, 852, 692, 3) (553, 1080, 808) (553, 852, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 4_overlay.mp4\n",
      "(488, 1080, 858, 3) (488, 855, 730, 3) (488, 1080, 808) (488, 855, 730)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 5_overlay.mp4\n",
      "(460, 1080, 858, 3) (460, 853, 691, 3) (460, 1080, 808) (460, 853, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (460, 854, 691) != target shape (460, 853, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 7_overlay.mp4\n",
      "[Patient 78b] 3 pares válidos\n",
      "(559, 1080, 858, 3) (559, 853, 729, 3) (559, 1080, 808) (559, 853, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (559, 854, 729) != target shape (559, 853, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 3_overlay.mp4\n",
      "(482, 1080, 858, 3) (482, 855, 691, 3) (482, 1080, 808) (482, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 6_overlay.mp4\n",
      "(641, 1080, 858, 3) (641, 855, 729, 3) (641, 1080, 808) (641, 855, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 8_overlay.mp4\n",
      "[Patient 82] 2 pares válidos\n",
      "(275, 1080, 858, 3) (275, 848, 691, 3) (275, 1080, 808) (275, 848, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 82_ 2_overlay.mp4\n",
      "(284, 1080, 858, 3) (284, 850, 692, 3) (284, 1080, 808) (284, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (284, 851, 692) != target shape (284, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 82_ 8_overlay.mp4\n",
      "[Patient 83] 4 pares válidos\n",
      "(391, 1080, 858, 3) (391, 854, 691, 3) (391, 1080, 808) (391, 854, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 1_overlay.mp4\n",
      "(507, 1080, 858, 3) (507, 784, 727, 3) (507, 1080, 808) (507, 784, 727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (507, 785, 727) != target shape (507, 784, 727). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 3_overlay.mp4\n",
      "(590, 1080, 858, 3) (590, 849, 772, 3) (590, 1080, 808) (590, 849, 772)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (590, 850, 772) != target shape (590, 849, 772). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 6_overlay.mp4\n",
      "(689, 1080, 858, 3) (689, 856, 692, 3) (689, 1080, 808) (689, 856, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (689, 857, 692) != target shape (689, 856, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 7_overlay.mp4\n",
      "[Patient 84] 4 pares válidos\n",
      "(331, 1080, 854, 3) (331, 830, 690, 3) (331, 1080, 804) (331, 830, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 2_overlay.mp4\n",
      "(267, 1080, 854, 3) (267, 843, 691, 3) (267, 1080, 804) (267, 843, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 3_overlay.mp4\n",
      "(312, 1080, 854, 3) (312, 815, 690, 3) (312, 1080, 804) (312, 815, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 6_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 841, 690, 3) (242, 1080, 804) (242, 841, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (242, 842, 690) != target shape (242, 841, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 7_overlay.mp4\n",
      "[Patient 85] 4 pares válidos\n",
      "(496, 1080, 858, 3) (496, 854, 691, 3) (496, 1080, 808) (496, 854, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 1_overlay.mp4\n",
      "(714, 1080, 858, 3) (714, 853, 693, 3) (714, 1080, 808) (714, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (714, 854, 693) != target shape (714, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 4_overlay.mp4\n",
      "(706, 1080, 858, 3) (706, 855, 694, 3) (706, 1080, 808) (706, 855, 694)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 5_overlay.mp4\n",
      "(684, 1080, 858, 3) (684, 851, 729, 3) (684, 1080, 808) (684, 851, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 7_overlay.mp4\n",
      "[Patient 87] 4 pares válidos\n",
      "(325, 1080, 858, 3) (325, 853, 690, 3) (325, 1080, 808) (325, 853, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (325, 854, 690) != target shape (325, 853, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 1_overlay.mp4\n",
      "(272, 1080, 858, 3) (272, 855, 691, 3) (272, 1080, 808) (272, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 4_overlay.mp4\n",
      "(282, 1080, 858, 3) (282, 857, 691, 3) (282, 1080, 808) (282, 857, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (282, 858, 691) != target shape (282, 857, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 5_overlay.mp4\n",
      "(276, 1080, 858, 3) (276, 851, 691, 3) (276, 1080, 808) (276, 851, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 7_overlay.mp4\n",
      "[Patient 88] 4 pares válidos\n",
      "(434, 1080, 858, 3) (434, 844, 692, 3) (434, 1080, 808) (434, 844, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 1_overlay.mp4\n",
      "(439, 1080, 858, 3) (439, 851, 692, 3) (439, 1080, 808) (439, 851, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 4_overlay.mp4\n",
      "(470, 1080, 858, 3) (470, 850, 692, 3) (470, 1080, 808) (470, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (470, 851, 692) != target shape (470, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 6_overlay.mp4\n",
      "(449, 1080, 858, 3) (449, 847, 690, 3) (449, 1080, 808) (449, 847, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 7_overlay.mp4\n",
      "[Patient 89] 4 pares válidos\n",
      "(605, 1080, 858, 3) (605, 854, 692, 3) (605, 1080, 808) (605, 854, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 1_overlay.mp4\n",
      "(764, 1080, 858, 3) (764, 856, 808, 3) (764, 1080, 808) (764, 856, 808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (764, 857, 808) != target shape (764, 856, 808). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 4_overlay.mp4\n",
      "(744, 1080, 858, 3) (744, 852, 691, 3) (744, 1080, 808) (744, 852, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 6_overlay.mp4\n",
      "(670, 1080, 858, 3) (670, 858, 692, 3) (670, 1080, 808) (670, 858, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 7_overlay.mp4\n",
      "[Patient 92] 4 pares válidos\n",
      "(259, 1080, 858, 3) (259, 851, 694, 3) (259, 1080, 808) (259, 851, 694)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 1_overlay.mp4\n",
      "(252, 1080, 858, 3) (252, 857, 691, 3) (252, 1080, 808) (252, 857, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (252, 858, 691) != target shape (252, 857, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 4_overlay.mp4\n",
      "(239, 1080, 858, 3) (239, 856, 729, 3) (239, 1080, 808) (239, 856, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 857, 729) != target shape (239, 856, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 6_overlay.mp4\n",
      "(249, 1080, 858, 3) (249, 854, 692, 3) (249, 1080, 808) (249, 854, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 854, 692) != target shape (249, 854, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 8_overlay.mp4\n",
      "[Patient 93] 4 pares válidos\n",
      "(571, 1080, 858, 3) (571, 855, 691, 3) (571, 1080, 808) (571, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 2_overlay.mp4\n",
      "(386, 1080, 858, 3) (386, 851, 691, 3) (386, 1080, 808) (386, 851, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 4_overlay.mp4\n",
      "(392, 1080, 858, 3) (392, 848, 729, 3) (392, 1080, 808) (392, 848, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (393, 848, 729) != target shape (392, 848, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 6_overlay.mp4\n",
      "(598, 1080, 858, 3) (598, 851, 693, 3) (598, 1080, 808) (598, 851, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 7_overlay.mp4\n",
      "[Patient 94a] 4 pares válidos\n",
      "(430, 1080, 858, 3) (430, 700, 729, 3) (430, 1080, 808) (430, 700, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 1_overlay.mp4\n",
      "(380, 1080, 858, 3) (380, 653, 690, 3) (380, 1080, 808) (380, 653, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 3_overlay.mp4\n",
      "(423, 1080, 858, 3) (423, 669, 770, 3) (423, 1080, 808) (423, 669, 770)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 6_overlay.mp4\n",
      "(448, 1080, 858, 3) (448, 745, 729, 3) (448, 1080, 808) (448, 745, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 7_overlay.mp4\n",
      "[Patient 95] 4 pares válidos\n",
      "(244, 1080, 858, 3) (244, 862, 693, 3) (244, 1080, 808) (244, 862, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 1_overlay.mp4\n",
      "(286, 1080, 858, 3) (286, 854, 691, 3) (286, 1080, 808) (286, 854, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 4_overlay.mp4\n",
      "(237, 1080, 858, 3) (237, 855, 691, 3) (237, 1080, 808) (237, 855, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 855, 691) != target shape (237, 855, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 5_overlay.mp4\n",
      "(312, 1080, 858, 3) (312, 858, 767, 3) (312, 1080, 808) (312, 858, 767)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 7_overlay.mp4\n",
      "[Patient 96] 4 pares válidos\n",
      "(468, 1080, 858, 3) (468, 851, 691, 3) (468, 1080, 808) (468, 851, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 2_overlay.mp4\n",
      "(483, 1080, 858, 3) (483, 632, 686, 3) (483, 1080, 808) (483, 632, 686)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 4_overlay.mp4\n",
      "(540, 1080, 858, 3) (540, 847, 690, 3) (540, 1080, 808) (540, 847, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 5_overlay.mp4\n",
      "(454, 1080, 858, 3) (454, 792, 808, 3) (454, 1080, 808) (454, 792, 808)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 7_overlay.mp4\n",
      "[Patient 97] 4 pares válidos\n",
      "(459, 1080, 858, 3) (459, 849, 753, 3) (562, 1080, 808) (562, 849, 753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (562, 850, 753) != target shape (562, 849, 753). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Overlay fallo subject=Patient 97 idx=1: \n",
      "(545, 1080, 858, 3) (545, 751, 771, 3) (545, 1080, 808) (545, 751, 771)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 2_overlay.mp4\n",
      "(602, 1080, 858, 3) (602, 787, 690, 3) (602, 1080, 808) (602, 787, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 4_overlay.mp4\n",
      "(526, 1080, 858, 3) (526, 850, 691, 3) (526, 1080, 808) (526, 850, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (526, 851, 691) != target shape (526, 850, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 8_overlay.mp4\n",
      "[Patient 98] 4 pares válidos\n",
      "(284, 1080, 858, 3) (284, 856, 693, 3) (284, 1080, 808) (284, 856, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (284, 857, 693) != target shape (284, 856, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 1_overlay.mp4\n",
      "(282, 1080, 858, 3) (282, 855, 691, 3) (282, 1080, 808) (282, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 4_overlay.mp4\n",
      "(238, 1080, 858, 3) (238, 862, 693, 3) (238, 1080, 808) (238, 862, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 5_overlay.mp4\n",
      "(290, 1080, 858, 3) (290, 856, 691, 3) (290, 1080, 808) (290, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (290, 857, 691) != target shape (290, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 7_overlay.mp4\n",
      "[Patient 99] 4 pares válidos\n",
      "(267, 1080, 858, 3) (267, 852, 693, 3) (267, 1080, 808) (267, 852, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 1_overlay.mp4\n",
      "(259, 1080, 858, 3) (259, 853, 692, 3) (259, 1080, 808) (259, 853, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (259, 854, 692) != target shape (259, 853, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 3_overlay.mp4\n",
      "(248, 1080, 858, 3) (248, 856, 691, 3) (248, 1080, 808) (248, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (248, 857, 691) != target shape (248, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 5_overlay.mp4\n",
      "(343, 1080, 858, 3) (343, 855, 691, 3) (343, 1080, 808) (343, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 8_overlay.mp4\n",
      "[Patient 102] 4 pares válidos\n",
      "(270, 1080, 858, 3) (270, 804, 686, 3) (270, 1080, 808) (270, 804, 686)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 2_overlay.mp4\n",
      "(253, 1080, 858, 3) (253, 854, 691, 3) (253, 1080, 808) (253, 854, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (254, 854, 691) != target shape (253, 854, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 3_overlay.mp4\n",
      "(244, 1080, 858, 3) (244, 852, 689, 3) (244, 1080, 808) (244, 852, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 5_overlay.mp4\n",
      "(237, 1080, 858, 3) (237, 850, 692, 3) (237, 1080, 808) (237, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 851, 692) != target shape (237, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 7_overlay.mp4\n",
      "[Patient 103] 4 pares válidos\n",
      "(402, 1080, 858, 3) (402, 851, 692, 3) (402, 1080, 808) (402, 851, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 1_overlay.mp4\n",
      "(622, 1080, 858, 3) (622, 850, 692, 3) (622, 1080, 808) (622, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (622, 851, 692) != target shape (622, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 3_overlay.mp4\n",
      "(557, 1080, 858, 3) (557, 856, 692, 3) (557, 1080, 808) (557, 856, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (557, 857, 692) != target shape (557, 856, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 5_overlay.mp4\n",
      "(607, 1080, 858, 3) (607, 854, 689, 3) (607, 1080, 808) (607, 854, 689)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 8_overlay.mp4\n",
      "[Patient 104] 1 pares válidos\n",
      "(681, 1080, 858, 3) (681, 855, 691, 3) (681, 1080, 808) (681, 855, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 104_ 7_overlay.mp4\n",
      "[Patient 106] 4 pares válidos\n",
      "(221, 1080, 858, 3) (221, 859, 729, 3) (221, 1080, 808) (221, 859, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 2_overlay.mp4\n",
      "(246, 1080, 858, 3) (246, 858, 696, 3) (246, 1080, 808) (246, 858, 696)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 4_overlay.mp4\n",
      "(266, 1080, 858, 3) (266, 856, 691, 3) (266, 1080, 808) (266, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (266, 857, 691) != target shape (266, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 6_overlay.mp4\n",
      "(239, 1080, 858, 3) (239, 855, 764, 3) (239, 1080, 808) (239, 855, 764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 855, 764) != target shape (239, 855, 764). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 8_overlay.mp4\n",
      "[Patient 108] 4 pares válidos\n",
      "(737, 1080, 858, 3) (737, 786, 671, 3) (737, 1080, 808) (737, 786, 671)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (738, 786, 671) != target shape (737, 786, 671). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 2_overlay.mp4\n",
      "(609, 1080, 858, 3) (609, 843, 690, 3) (609, 1080, 808) (609, 843, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 4_overlay.mp4\n",
      "(772, 1080, 858, 3) (772, 838, 690, 3) (772, 1080, 808) (772, 838, 690)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 6_overlay.mp4\n",
      "(670, 1080, 858, 3) (670, 747, 808, 3) (670, 1080, 808) (670, 747, 808)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 8_overlay.mp4\n",
      "[Patient 110] 1 pares válidos\n",
      "(203, 1080, 858, 3) (203, 850, 692, 3) (203, 1080, 808) (203, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (203, 851, 692) != target shape (203, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 110_ 5_overlay.mp4\n",
      "[Patient 111] 4 pares válidos\n",
      "(196, 1080, 858, 3) (196, 855, 692, 3) (196, 1080, 808) (196, 855, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (197, 855, 692) != target shape (196, 855, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 2_overlay.mp4\n",
      "(208, 1080, 858, 3) (208, 858, 691, 3) (208, 1080, 808) (208, 858, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 4_overlay.mp4\n",
      "(173, 1080, 858, 3) (173, 853, 693, 3) (173, 1080, 808) (173, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (173, 854, 693) != target shape (173, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 5_overlay.mp4\n",
      "(245, 1080, 858, 3) (245, 860, 691, 3) (245, 1080, 808) (245, 860, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 7_overlay.mp4\n",
      "[Patient 112] 4 pares válidos\n",
      "(547, 1080, 858, 3) (547, 855, 693, 3) (547, 1080, 808) (547, 855, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 2_overlay.mp4\n",
      "(624, 1080, 858, 3) (624, 850, 692, 3) (624, 1080, 808) (624, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (624, 851, 692) != target shape (624, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 4_overlay.mp4\n",
      "(533, 1080, 858, 3) (533, 854, 693, 3) (533, 1080, 808) (533, 854, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 5_overlay.mp4\n",
      "(618, 1080, 858, 3) (618, 855, 693, 3) (618, 1080, 808) (618, 855, 693)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 8_overlay.mp4\n",
      "[Patient 113] 4 pares válidos\n",
      "(237, 1080, 858, 3) (237, 854, 691, 3) (237, 1080, 808) (237, 854, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 854, 691) != target shape (237, 854, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 2_overlay.mp4\n",
      "(293, 1080, 858, 3) (293, 855, 692, 3) (293, 1080, 808) (293, 855, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 4_overlay.mp4\n",
      "(235, 1080, 858, 3) (235, 856, 689, 3) (235, 1080, 808) (235, 856, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (235, 857, 689) != target shape (235, 856, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 6_overlay.mp4\n",
      "(285, 1080, 858, 3) (285, 851, 691, 3) (285, 1080, 808) (285, 851, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 8_overlay.mp4\n",
      "[Patient 115a] 4 pares válidos\n",
      "(259, 1080, 858, 3) (259, 851, 692, 3) (259, 1080, 808) (259, 851, 692)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 2_overlay.mp4\n",
      "(209, 1080, 858, 3) (209, 855, 764, 3) (209, 1080, 808) (209, 855, 764)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 3_overlay.mp4\n",
      "(187, 1080, 858, 3) (187, 855, 729, 3) (187, 1080, 808) (187, 855, 729)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 5_overlay.mp4\n",
      "(241, 1080, 858, 3) (241, 853, 693, 3) (241, 1080, 808) (241, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (241, 854, 693) != target shape (241, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 8_overlay.mp4\n",
      "[Patient 117] 4 pares válidos\n",
      "(446, 1080, 858, 3) (446, 855, 694, 3) (446, 1080, 808) (446, 855, 694)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 2_overlay.mp4\n",
      "(504, 1080, 858, 3) (504, 859, 691, 3) (504, 1080, 808) (504, 859, 691)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 4_overlay.mp4\n",
      "(614, 1080, 858, 3) (614, 855, 761, 3) (614, 1080, 808) (614, 855, 761)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 5_overlay.mp4\n",
      "(526, 1080, 858, 3) (526, 855, 808, 3) (526, 1080, 808) (526, 855, 808)\n",
      "🎥 guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 8_overlay.mp4\n",
      "Total patients processed: 65\n",
      "Total raw patients: 107\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import natsort\n",
    "import numpy as np\n",
    "from torchmetrics import Dice, JaccardIndex, Precision, Recall, Specificity, Accuracy\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import skvideo.io as sio\n",
    "import random\n",
    "import torchio as tio\n",
    "import skimage.morphology as skm\n",
    "from skimage import measure\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from miseval import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "\n",
    "\n",
    "def draw_label(img_rgb, text, org, font_scale=0.8, color=(255,255,255)):\n",
    "    cv2.putText(img_rgb, text, org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(img_rgb, text, org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def overlay_mask_on_frame(frame_rgb, mask_2d, color=(0,255,0), alpha=0.35, draw_contour=True):\n",
    "    out = frame_rgb.copy()\n",
    "    m = mask_2d.astype(bool)\n",
    "    if m.any():\n",
    "        color_arr = np.array(color, dtype=np.uint8)\n",
    "        out[m] = (out[m] * (1 - alpha) + color_arr * alpha).astype(np.uint8)\n",
    "        if draw_contour:\n",
    "            edges = find_boundaries(mask_2d, mode=\"inner\")\n",
    "            out[edges] = color_arr\n",
    "    return out\n",
    "\n",
    "def make_even_frame(img_rgb):\n",
    "    \"\"\"Asegura H y W pares para yuv420p con padding de 1 px si hace falta.\"\"\"\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    if pad_h or pad_w:\n",
    "        img_rgb = np.pad(img_rgb, ((0, pad_h), (0, pad_w), (0, 0)), mode='edge')\n",
    "    # garantizar memoria contigua y dtype correcto\n",
    "    return np.ascontiguousarray(img_rgb.astype(np.uint8))\n",
    "\n",
    "def write_triptych_skvideo(orig_DHWC, gt_DHW, pred_DHW, out_path, fps=25.0, method=\"ffmpegwriter\"):\n",
    "    D, H, W, _ = orig_DHWC.shape\n",
    "    assert gt_DHW.shape == (D, H, W) and pred_DHW.shape == (D, H, W)\n",
    "\n",
    "    if method == \"vwrite\":\n",
    "        frames = []\n",
    "        for t in range(D):\n",
    "            f0 = orig_DHWC[t]\n",
    "            f1 = overlay_mask_on_frame(f0, gt_DHW[t],  color=(0,255,0))\n",
    "            f2 = overlay_mask_on_frame(f0, pred_DHW[t], color=(255,0,255))\n",
    "            trip = np.concatenate([f0, f1, f2], axis=1)\n",
    "            draw_label(trip, \"Original\", (15, 34))\n",
    "            draw_label(trip, \"GT\",       (15 + W, 34))\n",
    "            draw_label(trip, \"Pred\",     (15 + 2*W, 34))\n",
    "            trip = make_even_frame(trip)\n",
    "            frames.append(trip)\n",
    "        vid = np.stack(frames, axis=0)\n",
    "        sio.vwrite(out_path, vid, outputdict={'-r': str(fps), '-vcodec': 'libx264', '-pix_fmt': 'yuv420p'})\n",
    "    else:\n",
    "        writer = sio.FFmpegWriter(out_path, outputdict={'-r': str(fps), '-vcodec': 'libx264', '-pix_fmt': 'yuv420p'})\n",
    "        for t in range(D):\n",
    "            f0 = orig_DHWC[t]\n",
    "            f1 = overlay_mask_on_frame(f0, gt_DHW[t],  color=(0,255,0))\n",
    "            f2 = overlay_mask_on_frame(f0, pred_DHW[t], color=(255,0,255))\n",
    "            trip = np.concatenate([f0, f1, f2], axis=1)\n",
    "            draw_label(trip, \"Original\", (15, 34))\n",
    "            draw_label(trip, \"GT\",       (15 + W, 34))\n",
    "            draw_label(trip, \"Pred\",     (15 + 2*W, 34))\n",
    "            trip = make_even_frame(trip)\n",
    "            writer.writeFrame(trip)\n",
    "        writer.close()\n",
    "    print(f\"🎥 guardado: {out_path}\")\n",
    "\n",
    "# === Resize de label map con NN a 128³ ===\n",
    "def resize_pred_n(label_vol_DHW,D, H, W):\n",
    "    # TorchIO espera (C, D, H, W); usamos C=1\n",
    "    lbl = label_vol_DHW.astype(np.int8)  # o uint8/uint16 según tus clases\n",
    "    img = tio.LabelMap(tensor=lbl[None, ...])  # (1, D, H, W)\n",
    "    img = tio.Resize((D,H,W))(img)     # NN por ser LabelMap\n",
    "    out = img.data.numpy()[0]                  # (D, H, W)\n",
    "    return out\n",
    "\n",
    "\n",
    "output_dir = \"/data/GitHub/Breast-AI-model/src/overlay_test_rc2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def safe_eval(gt, pred, metric):\n",
    "    try:\n",
    "        return evaluate(gt, pred, metric=metric)\n",
    "    except Exception as e:\n",
    "        # opcional: print(f\"[WARN] {metric} falló: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Acumuladores\n",
    "per_video_rows = []\n",
    "\n",
    "subfolders_ids = natsort.natsorted(get_subfolder_ids('/data/GitHub/Breast-AI-model/src/predictions_rc2/'))\n",
    "count_patients = 0\n",
    "\n",
    "for subfolder_id in subfolders_ids:\n",
    "    ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'FINAL LABELS')\n",
    "    if not os.path.exists(ground_truth_folder):\n",
    "        ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'Segmentations')\n",
    "        if not os.path.exists(ground_truth_folder):\n",
    "            continue\n",
    "    count_patients += 1\n",
    "\n",
    "    ground_truth_files = glob.glob(os.path.join(ground_truth_folder, '**', '*.mat'), recursive=True)\n",
    "    ground_truth_files = natsort.natsorted([f for f in ground_truth_files\n",
    "                                            if 'groundTruthMed.mat' not in f and 'Session.mat' not in f])\n",
    "\n",
    "    pred_files = glob.glob(os.path.join('/data/GitHub/Breast-AI-model/src/predictions_rc2', subfolder_id, '*.npy'))\n",
    "    pred_files = natsort.natsorted(pred_files)\n",
    "\n",
    "    patterns = [\n",
    "    os.path.join(videos_path, subfolder_id, 'VSI 8 [cC]lips', '*.[mM][pP]4'),\n",
    "    os.path.join(videos_path, subfolder_id, '8 VSI [cC]lips', '*.[mM][pP]4'),  # opcional, por si existe esa variante\n",
    "]\n",
    "\n",
    "    videos_files = []\n",
    "    for pat in patterns:\n",
    "        videos_files += glob.glob(pat)\n",
    "\n",
    "    # quitar duplicados y ordenar de forma natural\n",
    "    videos_files = natsort.natsorted({os.path.realpath(p) for p in videos_files})\n",
    "    # idx -> path\n",
    "    npy_by_idx = {get_npy_idx(p): p for p in pred_files if get_npy_idx(p) is not None}\n",
    "    videos_by_idx = {get_video_idx(v): v for v in videos_files if get_video_idx(v) is not None}\n",
    "\n",
    "    # (gt_mat, pred_npy, video_mp4, idx)\n",
    "    pairs = []\n",
    "    for mp in ground_truth_files:\n",
    "        i = get_mat_idx(mp)\n",
    "        if i is None:\n",
    "            continue\n",
    "        pred_p = npy_by_idx.get(i)\n",
    "        vid_p  = videos_by_idx.get(i)\n",
    "        if pred_p is not None and vid_p is not None:   # si no hay video, omitimos porque el ROI depende del video\n",
    "            pairs.append((mp, pred_p, vid_p, i))\n",
    "\n",
    "    print(f\"[{subfolder_id}] {len(pairs)} pares válidos\")\n",
    "\n",
    "    # ---- Métricas por video ----\n",
    "    # Por cada par\n",
    "    for gt_file, pred_file, vid_file, idx in pairs:\n",
    "        try:\n",
    "            video = sio.vread(vid_file)  # (D,H,W,3)\n",
    "            \n",
    "            cropped_video, roi_coords = process_video_and_get_crop(video)      # (D',H',W',3)\n",
    "            gt_DHW = np.transpose(loadmat(gt_file)['labels'], (2,0,1))      # (D,H,W)\n",
    "            minr, maxr, minc, maxc = roi_coords\n",
    "            trim_right_px=50\n",
    "\n",
    "            # 2) Aplicar recorte de borde derecho también al GT\n",
    "            if trim_right_px and gt_DHW.shape[2] > trim_right_px:\n",
    "                gt_DHW = gt_DHW[:, :, 0:-trim_right_px]\n",
    "\n",
    "            # 3) Recortar GT con el mismo ROI del video\n",
    "            gt_bin = gt_DHW[:, minr:maxr, minc:maxc]\n",
    "\n",
    "            print(video.shape, cropped_video.shape, gt_DHW.shape, gt_bin.shape)\n",
    "\n",
    "\n",
    "            pred = np.load(pred_file)                                   # (128,128,128)\n",
    "            pred_bin = (pred > 0).astype(np.uint8)\n",
    "            pred_rs  = resize_pred_n(pred_bin, gt_bin.shape[0], gt_bin.shape[1], gt_bin.shape[2])   # (D',H',W')\n",
    "\n",
    "            fps = get_fps(vid_file, fallback=25.0)\n",
    "            out_name = f\"{subfolder_id}_{idx:2d}_overlay.mp4\"\n",
    "            out_path = os.path.join(output_dir, out_name)\n",
    "\n",
    "            # Usa vwrite si lo quieres explícito: method=\"vwrite\"\n",
    "            write_triptych_skvideo(cropped_video, gt_bin, pred_rs, out_path, fps=fps, method=\"ffmpegwriter\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Overlay fallo subject={subfolder_id} idx={idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "print(f\"Total patients processed: {count_patients}\")\n",
    "print(f\"Total raw patients: {len(subfolders_ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligthning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
