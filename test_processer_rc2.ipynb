{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350d9eff",
   "metadata": {},
   "source": [
    "# Test de metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cfa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Patient 2] 4 pares vÃ¡lidos\n",
      "[Patient 3] 4 pares vÃ¡lidos\n",
      "El recorte del GT (393, 1047, 317) no coincide con el del video (393, 1075, 865, 3)\n",
      "El recorte del GT (364, 1049, 318) no coincide con el del video (364, 1077, 866, 3)\n",
      "El recorte del GT (244, 1049, 317) no coincide con el del video (244, 1070, 860, 3)\n",
      "El recorte del GT (305, 1048, 317) no coincide con el del video (305, 1076, 865, 3)\n",
      "[Patient 9] 4 pares vÃ¡lidos\n",
      "El recorte del GT (291, 1059, 316) no coincide con el del video (291, 1075, 859, 3)\n",
      "El recorte del GT (331, 1059, 317) no coincide con el del video (331, 1075, 866, 3)\n",
      "El recorte del GT (297, 1058, 317) no coincide con el del video (297, 1074, 866, 3)\n",
      "El recorte del GT (289, 1060, 317) no coincide con el del video (289, 1074, 860, 3)\n",
      "[Patient 12] 4 pares vÃ¡lidos\n",
      "[Patient 13] 4 pares vÃ¡lidos\n",
      "El recorte del GT (301, 1052, 318) no coincide con el del video (301, 1075, 861, 3)\n",
      "El recorte del GT (201, 1017, 318) no coincide con el del video (201, 1017, 860, 3)\n",
      "El recorte del GT (185, 1053, 318) no coincide con el del video (185, 1076, 860, 3)\n",
      "El recorte del GT (271, 1054, 318) no coincide con el del video (271, 1077, 859, 3)\n",
      "[Patient 18] 4 pares vÃ¡lidos\n",
      "[Patient 19] 4 pares vÃ¡lidos\n",
      "[Patient 22] 4 pares vÃ¡lidos\n",
      "[Patient 23] 4 pares vÃ¡lidos\n",
      "[Patient 24] 4 pares vÃ¡lidos\n",
      "[Patient 25] 4 pares vÃ¡lidos\n",
      "[Patient 27] 4 pares vÃ¡lidos\n",
      "El recorte del GT (338, 888, 323) no coincide con el del video (338, 1075, 861, 3)\n",
      "El recorte del GT (355, 876, 323) no coincide con el del video (355, 1038, 860, 3)\n",
      "El recorte del GT (290, 888, 323) no coincide con el del video (290, 1075, 860, 3)\n",
      "El recorte del GT (371, 888, 323) no coincide con el del video (371, 1075, 865, 3)\n",
      "[Patient 32] 4 pares vÃ¡lidos\n",
      "[Patient 33] 4 pares vÃ¡lidos\n",
      "[Patient 35] 4 pares vÃ¡lidos\n",
      "[Patient 37a] 4 pares vÃ¡lidos\n",
      "[Patient 39] 4 pares vÃ¡lidos\n",
      "El recorte del GT (404, 1058, 323) no coincide con el del video (404, 1075, 859, 3)\n",
      "El recorte del GT (330, 1057, 323) no coincide con el del video (330, 1074, 865, 3)\n",
      "El recorte del GT (289, 1050, 324) no coincide con el del video (289, 1067, 866, 3)\n",
      "El recorte del GT (321, 1063, 323) no coincide con el del video (321, 1080, 865, 3)\n",
      "[Patient 41] 4 pares vÃ¡lidos\n",
      "[Patient 42] 4 pares vÃ¡lidos\n",
      "[Patient 49] 4 pares vÃ¡lidos\n",
      "[Patient 52] 4 pares vÃ¡lidos\n",
      "[Patient 54] 4 pares vÃ¡lidos\n",
      "[Patient 58] 4 pares vÃ¡lidos\n",
      "[Patient 59] 4 pares vÃ¡lidos\n",
      "[Patient 61] 4 pares vÃ¡lidos\n",
      "[Patient 62] 4 pares vÃ¡lidos\n",
      "[Patient 63] 4 pares vÃ¡lidos\n",
      "[Patient 64] 4 pares vÃ¡lidos\n",
      "[Patient 66] 4 pares vÃ¡lidos\n",
      "[Patient 67a] 4 pares vÃ¡lidos\n",
      "[Patient 67b] 4 pares vÃ¡lidos\n",
      "[Patient 68] 4 pares vÃ¡lidos\n",
      "[Patient 69] 4 pares vÃ¡lidos\n",
      "[Patient 70] 4 pares vÃ¡lidos\n",
      "[Patient 71] 4 pares vÃ¡lidos\n",
      "[Patient 73] 4 pares vÃ¡lidos\n",
      "[Patient 76] 4 pares vÃ¡lidos\n",
      "[Patient 78a] 4 pares vÃ¡lidos\n",
      "[Patient 78b] 3 pares vÃ¡lidos\n",
      "[Patient 82] 2 pares vÃ¡lidos\n",
      "[Patient 83] 4 pares vÃ¡lidos\n",
      "[Patient 84] 4 pares vÃ¡lidos\n",
      "[Patient 85] 4 pares vÃ¡lidos\n",
      "[Patient 87] 4 pares vÃ¡lidos\n",
      "[Patient 88] 4 pares vÃ¡lidos\n",
      "[Patient 89] 4 pares vÃ¡lidos\n",
      "[Patient 92] 4 pares vÃ¡lidos\n",
      "[Patient 93] 4 pares vÃ¡lidos\n",
      "[Patient 94a] 4 pares vÃ¡lidos\n",
      "[Patient 95] 4 pares vÃ¡lidos\n",
      "[Patient 96] 4 pares vÃ¡lidos\n",
      "[Patient 97] 4 pares vÃ¡lidos\n",
      "El recorte del GT (562, 850, 692) no coincide con el del video (459, 850, 692, 3)\n",
      "[Patient 98] 4 pares vÃ¡lidos\n",
      "[Patient 99] 4 pares vÃ¡lidos\n",
      "[Patient 102] 4 pares vÃ¡lidos\n",
      "[Patient 103] 4 pares vÃ¡lidos\n",
      "[Patient 104] 1 pares vÃ¡lidos\n",
      "[Patient 106] 4 pares vÃ¡lidos\n",
      "[Patient 108] 4 pares vÃ¡lidos\n",
      "[Patient 110] 1 pares vÃ¡lidos\n",
      "[Patient 111] 4 pares vÃ¡lidos\n",
      "[Patient 112] 4 pares vÃ¡lidos\n",
      "[Patient 113] 4 pares vÃ¡lidos\n",
      "[Patient 115a] 4 pares vÃ¡lidos\n",
      "[Patient 117] 4 pares vÃ¡lidos\n",
      "âœ… Guardado: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_per_video.csv\n",
      "âœ… Guardado con etiquetas: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_per_patient_mean.csv\n",
      "âœ… Guardado: /data/GitHub/Breast-AI-model/src/metrics_rc2/metrics_dataset_mean.csv\n",
      "Total patients processed: 65\n",
      "Total raw patients: 107\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import natsort\n",
    "import numpy as np\n",
    "from torchmetrics import Dice, JaccardIndex, Precision, Recall, Specificity, Accuracy\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import skvideo.io as sio\n",
    "import random\n",
    "import torchio as tio\n",
    "import skimage.morphology as skm\n",
    "from skimage import measure\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from miseval import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "ground_truth_path = '/data/DATA/private_data/Breast studyrc2'\n",
    "videos_path = '/data/DATA/private_data/Breast studyrc2/'\n",
    "\n",
    "\n",
    "# --- regex y helpers ---\n",
    "mat_idx_re_sweep = re.compile(r'_sweep(\\d+)\\.mat$', re.IGNORECASE)\n",
    "mat_idx_re_dash  = re.compile(r'-(\\d+)\\.mat$', re.IGNORECASE)\n",
    "npy_idx_re       = re.compile(r'-(\\d+)_.*masken\\.npy$', re.IGNORECASE)\n",
    "video_idx_re     = re.compile(r'-(\\d+)\\.mp4$', re.IGNORECASE)  # videos: ...-#id.mp4\n",
    "\n",
    "def get_mat_idx(p):\n",
    "    base = os.path.basename(p)\n",
    "    m = mat_idx_re_sweep.search(base)\n",
    "    if m: return int(m.group(1))\n",
    "    m = mat_idx_re_dash.search(base)\n",
    "    if m: return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def get_npy_idx(p):\n",
    "    m = npy_idx_re.search(os.path.basename(p))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def get_video_idx(p):\n",
    "    m = video_idx_re.search(os.path.basename(p))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "#obtain subfolder ids in /data/GitHub/Breast-AI-model/src/predictions_rc2/\n",
    "def get_subfolder_ids(path):\n",
    "    subfolders = glob.glob(os.path.join(path, '*/'))\n",
    "    subfolder_ids = [os.path.basename(os.path.normpath(folder)) for folder in subfolders]\n",
    "    return natsort.natsorted(subfolder_ids)\n",
    "\n",
    "# === FunciÃ³n: obtiene ROI desde el video (igual que tu pipeline) ===\n",
    "def process_video_and_get_crop(video, min_obj_size=1000, hole_size=1000):\n",
    "    video = video[:, :, 0:-50, :]  # elimina borde derecho (artefacto)\n",
    "    D, H, W, _ = video.shape\n",
    "    ref_idx, idx1, idx2 = random.sample(range(D), 3)\n",
    "    diff1 = (video[ref_idx].astype(np.float32) - video[idx1].astype(np.float32)).astype(np.uint8)\n",
    "    diff2 = (video[ref_idx].astype(np.float32) - video[idx2].astype(np.float32)).astype(np.uint8)\n",
    "    gray1 = cv2.cvtColor(diff1, cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(diff2, cv2.COLOR_RGB2GRAY)\n",
    "    mask = np.logical_or(gray1 > 0, gray2 > 0)\n",
    "    mask = skm.binary_erosion(mask)\n",
    "    mask = skm.remove_small_objects(mask, min_size=min_obj_size)\n",
    "    mask = skm.remove_small_holes(mask, area_threshold=hole_size)\n",
    "    mask = skm.opening(mask)\n",
    "    labels = measure.label(mask)\n",
    "    props = measure.regionprops(labels)\n",
    "    if not props:\n",
    "        raise RuntimeError(\"No se encontrÃ³ ningÃºn objeto tras la limpieza.\")\n",
    "    largest = max(props, key=lambda p: p.area)\n",
    "    minr, minc, maxr, maxc = largest.bbox\n",
    "    cropped_video = video[:, minr:maxr, minc:maxc, :]\n",
    "    return cropped_video, (minr, maxr, minc, maxc)\n",
    "\n",
    "# === Resize de label map con NN a 128Â³ ===\n",
    "def resize_label_128(label_vol_DHW):\n",
    "    # TorchIO espera (C, D, H, W); usamos C=1\n",
    "    lbl = label_vol_DHW.astype(np.int16)  # o uint8/uint16 segÃºn tus clases\n",
    "    img = tio.LabelMap(tensor=lbl[None, ...])  # (1, D, H, W)\n",
    "    img = tio.Resize((128, 128, 128))(img)     # NN por ser LabelMap\n",
    "    out = img.data.numpy()[0]                  # (D, H, W)\n",
    "    return out\n",
    "\n",
    "def resize_video_128(video_DHWC):\n",
    "    # ConversiÃ³n a escala de grises y resize a (128,128,128)\n",
    "    _, H, W, _ = video_DHWC.shape\n",
    "    # gray_vol = np.zeros((1, D, H, W), dtype=np.uint8)\n",
    "    # for i in range(D):\n",
    "    #     gray = np.dot(video[i], [0.2989, 0.5870, 0.1140])\n",
    "    #     gray_vol[0, i] = gray.astype(np.uint8)\n",
    "    # coefs float32 para no caer en float64\n",
    "    coefs = np.array([0.2989, 0.5870, 0.1140], dtype=np.float32)\n",
    "    # â†’ esto te da float32\n",
    "    gray = np.tensordot(video_DHWC, coefs, axes=([3], [0]))\n",
    "\n",
    "    # normaliza en [0,1] y SIGUE siendo float32\n",
    "    gray = gray / np.float32(255.0)\n",
    "\n",
    "    # 3. Construye el ScalarImage YA en float32 normalizado\n",
    "    img = tio.ScalarImage(tensor=gray[None, ...])  # shape (1,D,H,W), dtype float32\n",
    "    #solo cambiar tamaÃ±o temporal a 128, manteniendo H W sin cambiar\n",
    "    img = tio.Resize((128, H, W))(img)\n",
    "    out = img.data.numpy()  # shape (1,128,128,128), dtype float32\n",
    "    return out\n",
    "\n",
    "def gt_to_128_from_video(gt_DHW, video_path, roi_coords=None, trim_right_px=50):\n",
    "    \"\"\"\n",
    "    gt_DHW: volumen GT (D,H,W)\n",
    "    video_path: mp4 correspondiente (mismo Ã­ndice)\n",
    "    roi_coords: (minr, maxr, minc, maxc) si ya lo tienes; si no, se calcula del video\n",
    "    \"\"\"\n",
    "    crop_flag=False\n",
    "    # 1) Cargar video y (si no hay coords) calcular ROI exactamente como en inferencia\n",
    "    video = sio.vread(video_path)  # (D,H,W,3)\n",
    "    if roi_coords is None:\n",
    "        cropped_video, roi_coords = process_video_and_get_crop(video)\n",
    "    minr, maxr, minc, maxc = roi_coords\n",
    "\n",
    "    # 2) Aplicar recorte de borde derecho tambiÃ©n al GT\n",
    "    if trim_right_px and gt_DHW.shape[2] > trim_right_px:\n",
    "        gt_DHW = gt_DHW[:, :, 0:-trim_right_px]\n",
    "\n",
    "    # 3) Recortar GT con el mismo ROI del video\n",
    "    gt_crop = gt_DHW[:, minr:maxr, minc:maxc]\n",
    "    if gt_crop.size != cropped_video[:,:,:,0].size:\n",
    "        crop_flag = True\n",
    "        print(f\"El recorte del GT {gt_crop.shape} no coincide con el del video {cropped_video.shape}\")\n",
    "        \n",
    "    # 4) Resize NN a 128Â³\n",
    "    gt_128 = resize_label_128(gt_crop)\n",
    "    cropped_video = resize_video_128(cropped_video)[0]  # (128, H', W', 3)\n",
    "    return gt_128, roi_coords,cropped_video,crop_flag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dir = \"/data/GitHub/Breast-AI-model/src/metrics_rc2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "metric_list = [\"DSC\", \"IoU\", \"AHD\", \"VS\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"AUC\",\"Kap\"]\n",
    "\n",
    "def safe_eval(gt, pred, metric):\n",
    "    try:\n",
    "        return evaluate(gt, pred, metric=metric)\n",
    "    except Exception as e:\n",
    "        # opcional: print(f\"[WARN] {metric} fallÃ³: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Acumuladores\n",
    "per_video_rows = []\n",
    "\n",
    "subfolders_ids = natsort.natsorted(get_subfolder_ids('/data/GitHub/Breast-AI-model/src/predictions_rc2/'))\n",
    "count_patients = 0\n",
    "\n",
    "for subfolder_id in subfolders_ids:\n",
    "    ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'FINAL LABELS')\n",
    "    if not os.path.exists(ground_truth_folder):\n",
    "        ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'Segmentations')\n",
    "        if not os.path.exists(ground_truth_folder):\n",
    "            continue\n",
    "    count_patients += 1\n",
    "\n",
    "    ground_truth_files = glob.glob(os.path.join(ground_truth_folder, '**', '*.mat'), recursive=True)\n",
    "    ground_truth_files = natsort.natsorted([f for f in ground_truth_files\n",
    "                                            if 'groundTruthMed.mat' not in f and 'Session.mat' not in f])\n",
    "\n",
    "    pred_files = glob.glob(os.path.join('/data/GitHub/Breast-AI-model/src/predictions_rc2', subfolder_id, '*.npy'))\n",
    "    pred_files = natsort.natsorted(pred_files)\n",
    "\n",
    "    patterns = [\n",
    "    os.path.join(videos_path, subfolder_id, 'VSI 8 [cC]lips', '*.[mM][pP]4'),\n",
    "    os.path.join(videos_path, subfolder_id, '8 VSI [cC]lips', '*.[mM][pP]4'),  # opcional, por si existe esa variante\n",
    "]\n",
    "\n",
    "    videos_files = []\n",
    "    for pat in patterns:\n",
    "        videos_files += glob.glob(pat)\n",
    "\n",
    "    # quitar duplicados y ordenar de forma natural\n",
    "    videos_files = natsort.natsorted({os.path.realpath(p) for p in videos_files})\n",
    "    # idx -> path\n",
    "    npy_by_idx = {get_npy_idx(p): p for p in pred_files if get_npy_idx(p) is not None}\n",
    "    videos_by_idx = {get_video_idx(v): v for v in videos_files if get_video_idx(v) is not None}\n",
    "\n",
    "    # (gt_mat, pred_npy, video_mp4, idx)\n",
    "    pairs = []\n",
    "    for mp in ground_truth_files:\n",
    "        i = get_mat_idx(mp)\n",
    "        if i is None:\n",
    "            continue\n",
    "        pred_p = npy_by_idx.get(i)\n",
    "        vid_p  = videos_by_idx.get(i)\n",
    "        if pred_p is not None and vid_p is not None:   # si no hay video, omitimos porque el ROI depende del video\n",
    "            pairs.append((mp, pred_p, vid_p, i))\n",
    "\n",
    "    print(f\"[{subfolder_id}] {len(pairs)} pares vÃ¡lidos\")\n",
    "\n",
    "    # ---- MÃ©tricas por video ----\n",
    "    for gt_file, pred_file, vid_file, idx in pairs:\n",
    "        try:\n",
    "            # carga GT, alinea y redimensiona\n",
    "            gt = np.transpose(loadmat(gt_file)['labels'], (2, 0, 1))  # (D,H,W)\n",
    "            gt_128, coords, cropped_video,crop_flag = gt_to_128_from_video(gt, vid_file)\n",
    "\n",
    "            # carga pred\n",
    "            pred = np.load(pred_file)\n",
    "\n",
    "            # binariza por seguridad\n",
    "            gt_bin   = (gt_128 > 0).astype(np.uint8)\n",
    "            pred_bin = (pred   > 0).astype(np.uint8)\n",
    "\n",
    "            # evalÃºa todas las mÃ©tricas (con tolerancia a fallos)\n",
    "            row = {\n",
    "                \"subject_id\": subfolder_id.split(' ')[-1],\n",
    "                \"sweep\": idx,\n",
    "                \"D\": gt_bin.shape[0], \"H\": gt_bin.shape[1], \"W\": gt_bin.shape[2],\n",
    "                'Crop flag': crop_flag\n",
    "            }\n",
    "            for m in metric_list:\n",
    "                row[m] = np.round(safe_eval(gt_bin, pred_bin, metric=m),2)\n",
    "\n",
    "            per_video_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Loguea y sigue con el siguiente video\n",
    "            print(f\"[WARN] FallÃ³ subject={subfolder_id} idx={idx}: {e}\")\n",
    "            continue\n",
    "# ---- DataFrames y CSVs ----\n",
    "df_videos = pd.DataFrame(per_video_rows)\n",
    "\n",
    "# CSV 1: mÃ©tricas individuales por video\n",
    "csv_per_video = os.path.join(output_dir, \"metrics_per_video.csv\")\n",
    "df_videos.to_csv(csv_per_video, index=False)\n",
    "print(f\"âœ… Guardado: {csv_per_video}\")\n",
    "\n",
    "# === NORMALIZAR IDs EN df_videos (crea un ID canÃ³nico numÃ©rico) ===\n",
    "# subject_id tÃ­picamente es \"Patient 2\", \"Patient 03\", etc. Extraemos el entero.\n",
    "df_videos['patient_num'] = (\n",
    "    df_videos['subject_id'].astype(str).str.extract(r'(\\d+)')[0].astype('Int64')\n",
    ")\n",
    "\n",
    "# CSV 2: promedios por paciente (ignora NaN) + Crop flag\n",
    "agg_cols = metric_list  # mÃ©tricas a promediar\n",
    "agg_dict = {m: 'mean' for m in agg_cols}\n",
    "agg_dict['Crop flag'] = 'max'   # True si algÃºn video del paciente tuvo Crop flag\n",
    "\n",
    "df_patients = (\n",
    "    df_videos\n",
    "    .groupby(['subject_id', 'patient_num'], dropna=False)\n",
    "    .agg(agg_dict)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ==== MERGE CON BS.csv (usa patient_num) ====\n",
    "bs_csv = '/data/GitHub/Breast-AI-model/src/BS.csv'  # ajusta si es necesario\n",
    "bs = pd.read_csv(bs_csv)\n",
    "\n",
    "# Normaliza nombres y tipos\n",
    "bs.columns = [c.strip() for c in bs.columns]\n",
    "esperadas = ['ID', 'Mass/no mass', 'Cancer/no cancer', 'Category']\n",
    "faltantes = [c for c in esperadas if c not in bs.columns]\n",
    "if faltantes:\n",
    "    raise ValueError(f\"Faltan columnas en BS.csv: {faltantes}\")\n",
    "\n",
    "# Crea tambiÃ©n el ID canÃ³nico en BS.csv\n",
    "bs['ID_str'] = bs['ID'].astype(str).str.strip()\n",
    "bs['patient_num'] = bs['ID_str'].str.extract(r'(\\d+)')[0].astype('Int64')\n",
    "bs = bs.drop_duplicates(subset='patient_num', keep='first')\n",
    "\n",
    "# Merge por patient_num\n",
    "df_patients_labeled = (\n",
    "    df_patients.merge(\n",
    "        bs[['patient_num','Mass/no mass','Cancer/no cancer','Category']],\n",
    "        on='patient_num', how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# (Opcional) mover columnas de etiqueta al inicio\n",
    "label_cols = ['Mass/no mass','Cancer/no cancer','Category']\n",
    "ordered_cols = (['subject_id','patient_num','Crop flag'] + label_cols +\n",
    "                [c for c in df_patients_labeled.columns if c not in (['subject_id','patient_num','Crop flag'] + label_cols)])\n",
    "df_patients_labeled = df_patients_labeled[ordered_cols]\n",
    "\n",
    "# Reporta pacientes sin match\n",
    "sin_match = df_patients_labeled[df_patients_labeled['Mass/no mass'].isna()]['subject_id'].tolist()\n",
    "if sin_match:\n",
    "    print(f\"[INFO] {len(sin_match)} pacientes sin match en BS.csv (ejemplos): {sin_match[:10]}\")\n",
    "\n",
    "\n",
    "csv_per_patient_labeled = os.path.join(output_dir, \"metrics_per_patient_mean.csv\")\n",
    "df_patients_labeled.set_index('subject_id').drop(columns=['patient_num']).to_csv(csv_per_patient_labeled)\n",
    "print(f\"âœ… Guardado con etiquetas: {csv_per_patient_labeled}\")\n",
    "\n",
    "# CSV 3: promedio global del dataset (ignora NaN)\n",
    "global_means = df_videos[agg_cols].mean(numeric_only=True)\n",
    "df_global = pd.DataFrame(global_means).T\n",
    "df_global.index = [\"dataset_mean\"]\n",
    "csv_global = os.path.join(output_dir, \"metrics_dataset_mean.csv\")\n",
    "df_global.to_csv(csv_global)\n",
    "print(f\"âœ… Guardado: {csv_global}\")\n",
    "\n",
    "\n",
    "        #Mostrar el primer frame no vacÃ­o del gt_128\n",
    "        #first_non_empty_frame = np.argmax(np.any(gt_128, axis=(1, 2)))\n",
    "        #mostrar el video, el ground_truth y la predicciÃ³n\n",
    "        # if first_non_empty_frame < gt_128.shape[0]:\n",
    "        #     plt.figure(figsize=(12, 4))\n",
    "        #     plt.subplot(1, 3, 1)\n",
    "        #     plt.imshow(gt_128[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title('Ground Truth Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(1, 3, 2)\n",
    "        #     plt.imshow(pred[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title('Prediction Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(1, 3, 3)\n",
    "        #     plt.imshow(cropped_video[first_non_empty_frame], cmap='gray')\n",
    "        #     plt.title  # 2) Aplicar recorte de borde derecho tambiÃ©n al GT\n",
    "        #     ('Video Frame')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.show()\n",
    "        #print(\"Shapes -> GT:\", ground_truth.shape, \"| Pred:\", prediction.shape, \"| Video:\", video.shape)\n",
    "  \n",
    "\n",
    "    #break\n",
    "print(f\"Total patients processed: {count_patients}\")\n",
    "print(f\"Total raw patients: {len(subfolders_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bd7a3",
   "metadata": {},
   "source": [
    "# Generacion de OVERLAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dbfa412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Patient 2] 4 pares vÃ¡lidos\n",
      "(273, 1080, 1854, 3) (273, 1076, 866, 3) (273, 1080, 1804) (273, 1076, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 1_overlay.mp4\n",
      "(361, 1080, 1854, 3) (361, 1075, 866, 3) (361, 1080, 1804) (361, 1075, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 4_overlay.mp4\n",
      "(301, 1080, 1854, 3) (301, 1077, 860, 3) (301, 1080, 1804) (301, 1077, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 5_overlay.mp4\n",
      "(332, 1080, 1854, 3) (332, 1075, 862, 3) (332, 1080, 1804) (332, 1075, 862)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 2_ 8_overlay.mp4\n",
      "[Patient 3] 4 pares vÃ¡lidos\n",
      "(393, 1080, 1854, 3) (393, 1077, 861, 3) (393, 1052, 811) (393, 1049, 318)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=2: \n",
      "(364, 1080, 1854, 3) (364, 1064, 859, 3) (364, 1052, 811) (364, 1036, 317)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=4: \n",
      "(244, 1080, 1854, 3) (244, 1073, 862, 3) (244, 1052, 811) (244, 1047, 319)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=6: \n",
      "(305, 1080, 1854, 3) (305, 1072, 866, 3) (305, 1052, 811) (305, 1048, 318)\n",
      "[WARN] Overlay fallo subject=Patient 3 idx=8: \n",
      "[Patient 9] 4 pares vÃ¡lidos\n",
      "(291, 1080, 1854, 3) (291, 1075, 861, 3) (291, 1064, 810) (291, 1059, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=1: \n",
      "(331, 1080, 1854, 3) (331, 1075, 864, 3) (331, 1064, 810) (331, 1059, 318)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=4: \n",
      "(297, 1080, 1854, 3) (297, 1075, 867, 3) (297, 1064, 810) (297, 1059, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=5: \n",
      "(289, 1080, 1854, 3) (289, 1067, 861, 3) (289, 1064, 810) (289, 1051, 317)\n",
      "[WARN] Overlay fallo subject=Patient 9 idx=8: \n",
      "[Patient 12] 4 pares vÃ¡lidos\n",
      "(363, 1080, 1854, 3) (363, 1073, 859, 3) (363, 1080, 1804) (363, 1073, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 1_overlay.mp4\n",
      "(306, 1080, 1854, 3) (306, 1077, 859, 3) (306, 1080, 1804) (306, 1077, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 4_overlay.mp4\n",
      "(265, 1080, 1854, 3) (265, 1075, 857, 3) (265, 1080, 1804) (265, 1075, 857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (265, 1075, 858) != target shape (265, 1075, 857). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 6_overlay.mp4\n",
      "(239, 1080, 1854, 3) (239, 842, 859, 3) (239, 1080, 1804) (239, 842, 859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 842, 859) != target shape (239, 842, 859). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 12_ 8_overlay.mp4\n",
      "[Patient 13] 4 pares vÃ¡lidos\n",
      "(301, 1080, 1854, 3) (301, 1067, 862, 3) (301, 1057, 812) (301, 1044, 319)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=1: \n",
      "(201, 1080, 1854, 3) (201, 1033, 861, 3) (201, 1057, 812) (201, 1033, 319)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=4: \n",
      "(185, 1080, 1854, 3) (185, 1075, 860, 3) (185, 1057, 812) (185, 1054, 318)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=6: \n",
      "(271, 1080, 1854, 3) (271, 1075, 860, 3) (271, 1057, 812) (271, 1054, 318)\n",
      "[WARN] Overlay fallo subject=Patient 13 idx=8: \n",
      "[Patient 18] 4 pares vÃ¡lidos\n",
      "(249, 1080, 1854, 3) (249, 1064, 832, 3) (249, 1080, 1804) (249, 1064, 832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 1064, 832) != target shape (249, 1064, 832). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 2_overlay.mp4\n",
      "(275, 1080, 1854, 3) (275, 488, 804, 3) (275, 1080, 1804) (275, 488, 804)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 4_overlay.mp4\n",
      "(293, 1080, 1854, 3) (293, 1077, 860, 3) (293, 1080, 1804) (293, 1077, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 5_overlay.mp4\n",
      "(232, 1080, 1854, 3) (232, 1076, 866, 3) (232, 1080, 1804) (232, 1076, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 18_ 7_overlay.mp4\n",
      "[Patient 19] 4 pares vÃ¡lidos\n",
      "(399, 1080, 1854, 3) (399, 1072, 860, 3) (399, 1080, 1804) (399, 1072, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 2_overlay.mp4\n",
      "(354, 1080, 1854, 3) (354, 1073, 860, 3) (354, 1080, 1804) (354, 1073, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 4_overlay.mp4\n",
      "(403, 1080, 1854, 3) (403, 1077, 866, 3) (403, 1080, 1804) (403, 1077, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 5_overlay.mp4\n",
      "(342, 1080, 1854, 3) (342, 1077, 865, 3) (342, 1080, 1804) (342, 1077, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 19_ 8_overlay.mp4\n",
      "[Patient 22] 4 pares vÃ¡lidos\n",
      "(431, 1080, 1854, 3) (431, 1058, 865, 3) (431, 1080, 1804) (431, 1058, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 1_overlay.mp4\n",
      "(447, 1080, 1854, 3) (447, 1072, 865, 3) (447, 1080, 1804) (447, 1072, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 4_overlay.mp4\n",
      "(477, 1080, 1854, 3) (477, 1071, 860, 3) (477, 1080, 1804) (477, 1071, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 6_overlay.mp4\n",
      "(443, 1080, 1854, 3) (443, 903, 864, 3) (443, 1080, 1804) (443, 903, 864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (444, 903, 864) != target shape (443, 903, 864). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 22_ 7_overlay.mp4\n",
      "[Patient 23] 4 pares vÃ¡lidos\n",
      "(572, 1080, 1854, 3) (572, 587, 834, 3) (572, 1080, 1804) (572, 587, 834)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (572, 587, 835) != target shape (572, 587, 834). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 2_overlay.mp4\n",
      "(429, 1080, 1854, 3) (429, 648, 865, 3) (429, 1080, 1804) (429, 648, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 4_overlay.mp4\n",
      "(780, 1080, 1854, 3) (780, 573, 837, 3) (780, 1080, 1804) (780, 573, 837)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 5_overlay.mp4\n",
      "(395, 1080, 1854, 3) (395, 789, 857, 3) (395, 1080, 1804) (395, 789, 857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (395, 789, 858) != target shape (395, 789, 857). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 23_ 7_overlay.mp4\n",
      "[Patient 24] 4 pares vÃ¡lidos\n",
      "(603, 1080, 1854, 3) (603, 1067, 860, 3) (603, 1080, 1804) (603, 1067, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 1_overlay.mp4\n",
      "(426, 1080, 1854, 3) (426, 1075, 860, 3) (426, 1080, 1804) (426, 1075, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 3_overlay.mp4\n",
      "(369, 1080, 1854, 3) (369, 1076, 859, 3) (369, 1080, 1804) (369, 1076, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 5_overlay.mp4\n",
      "(355, 1080, 1854, 3) (355, 1075, 866, 3) (355, 1080, 1804) (355, 1075, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 24_ 7_overlay.mp4\n",
      "[Patient 25] 4 pares vÃ¡lidos\n",
      "(569, 1080, 1854, 3) (569, 1075, 860, 3) (569, 1080, 1804) (569, 1075, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 1_overlay.mp4\n",
      "(474, 1080, 1854, 3) (474, 1073, 861, 3) (474, 1080, 1804) (474, 1073, 861)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (475, 1073, 861) != target shape (474, 1073, 861). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 3_overlay.mp4\n",
      "(293, 1080, 1854, 3) (293, 1067, 865, 3) (293, 1080, 1804) (293, 1067, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 6_overlay.mp4\n",
      "(416, 1080, 1854, 3) (416, 1060, 866, 3) (416, 1080, 1804) (416, 1060, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 25_ 8_overlay.mp4\n",
      "[Patient 27] 4 pares vÃ¡lidos\n",
      "(338, 1080, 1854, 3) (338, 1064, 865, 3) (338, 893, 817) (338, 877, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=2: \n",
      "(355, 1080, 1854, 3) (355, 1070, 865, 3) (355, 893, 817) (355, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=3: \n",
      "(290, 1080, 1854, 3) (290, 1075, 859, 3) (290, 893, 817) (290, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=6: \n",
      "(371, 1080, 1854, 3) (371, 1075, 860, 3) (371, 893, 817) (371, 888, 323)\n",
      "[WARN] Overlay fallo subject=Patient 27 idx=7: \n",
      "[Patient 32] 4 pares vÃ¡lidos\n",
      "(414, 1080, 1854, 3) (414, 1076, 860, 3) (414, 1080, 1804) (414, 1076, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 2_overlay.mp4\n",
      "(369, 1080, 1854, 3) (369, 1076, 865, 3) (369, 1080, 1804) (369, 1076, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 4_overlay.mp4\n",
      "(221, 1080, 1854, 3) (221, 1064, 865, 3) (221, 1080, 1804) (221, 1064, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 6_overlay.mp4\n",
      "(385, 1080, 1854, 3) (385, 1068, 865, 3) (385, 1080, 1804) (385, 1068, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 32_ 7_overlay.mp4\n",
      "[Patient 33] 4 pares vÃ¡lidos\n",
      "(320, 1080, 1854, 3) (320, 973, 865, 3) (320, 1080, 1804) (320, 973, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 1_overlay.mp4\n",
      "(249, 1080, 1854, 3) (249, 1064, 860, 3) (249, 1080, 1804) (249, 1064, 860)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 1064, 860) != target shape (249, 1064, 860). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 4_overlay.mp4\n",
      "(220, 1080, 1854, 3) (220, 1070, 866, 3) (220, 1080, 1804) (220, 1070, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 5_overlay.mp4\n",
      "(246, 1080, 1854, 3) (246, 1075, 859, 3) (246, 1080, 1804) (246, 1075, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 33_ 7_overlay.mp4\n",
      "[Patient 35] 4 pares vÃ¡lidos\n",
      "(378, 1080, 1854, 3) (378, 1075, 865, 3) (378, 1080, 1804) (378, 1075, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 1_overlay.mp4\n",
      "(241, 1080, 1854, 3) (241, 1073, 860, 3) (241, 1080, 1804) (241, 1073, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 4_overlay.mp4\n",
      "(175, 1080, 1854, 3) (175, 1064, 866, 3) (175, 1080, 1804) (175, 1064, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 6_overlay.mp4\n",
      "(303, 1080, 1854, 3) (303, 1076, 861, 3) (303, 1080, 1804) (303, 1076, 861)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 35_ 8_overlay.mp4\n",
      "[Patient 37a] 4 pares vÃ¡lidos\n",
      "(360, 1080, 1854, 3) (360, 1076, 867, 3) (360, 1080, 1804) (360, 1076, 867)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 1_overlay.mp4\n",
      "(311, 1080, 1854, 3) (311, 1071, 865, 3) (311, 1080, 1804) (311, 1071, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 4_overlay.mp4\n",
      "(302, 1080, 1854, 3) (302, 1075, 865, 3) (302, 1080, 1804) (302, 1075, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 6_overlay.mp4\n",
      "(353, 1080, 1854, 3) (353, 1066, 861, 3) (353, 1080, 1804) (353, 1066, 861)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 37a_ 8_overlay.mp4\n",
      "[Patient 39] 4 pares vÃ¡lidos\n",
      "(404, 1080, 1854, 3) (404, 1080, 865, 3) (404, 1063, 817) (404, 1063, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=2: \n",
      "(330, 1080, 1854, 3) (330, 1076, 866, 3) (330, 1063, 817) (330, 1059, 324)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=4: \n",
      "(289, 1080, 1854, 3) (289, 1076, 865, 3) (289, 1063, 817) (289, 1059, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=5: \n",
      "(321, 1080, 1854, 3) (321, 1073, 865, 3) (321, 1063, 817) (321, 1059, 323)\n",
      "[WARN] Overlay fallo subject=Patient 39 idx=8: \n",
      "[Patient 41] 4 pares vÃ¡lidos\n",
      "(403, 1080, 1854, 3) (403, 1076, 859, 3) (403, 1080, 1804) (403, 1076, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 2_overlay.mp4\n",
      "(429, 1080, 1854, 3) (429, 1076, 859, 3) (429, 1080, 1804) (429, 1076, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 4_overlay.mp4\n",
      "(306, 1080, 1854, 3) (306, 1077, 865, 3) (306, 1080, 1804) (306, 1077, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 5_overlay.mp4\n",
      "(349, 1080, 1854, 3) (349, 1072, 865, 3) (349, 1080, 1804) (349, 1072, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 41_ 8_overlay.mp4\n",
      "[Patient 42] 4 pares vÃ¡lidos\n",
      "(431, 1080, 854, 3) (431, 838, 691, 3) (431, 1080, 804) (431, 838, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 2_overlay.mp4\n",
      "(337, 1080, 854, 3) (337, 842, 689, 3) (337, 1080, 804) (337, 842, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 3_overlay.mp4\n",
      "(309, 1080, 854, 3) (309, 842, 690, 3) (309, 1080, 804) (309, 842, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 5_overlay.mp4\n",
      "(248, 1080, 854, 3) (248, 843, 691, 3) (248, 1080, 804) (248, 843, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 42_ 7_overlay.mp4\n",
      "[Patient 49] 4 pares vÃ¡lidos\n",
      "(196, 1080, 1854, 3) (196, 1075, 863, 3) (196, 1080, 1804) (196, 1075, 863)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (197, 1075, 863) != target shape (196, 1075, 863). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 2_overlay.mp4\n",
      "(258, 1080, 1854, 3) (258, 1075, 865, 3) (258, 1080, 1804) (258, 1075, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 4_overlay.mp4\n",
      "(270, 1080, 1854, 3) (270, 1064, 865, 3) (270, 1080, 1804) (270, 1064, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 5_overlay.mp4\n",
      "(192, 1080, 1854, 3) (192, 1065, 865, 3) (192, 1080, 1804) (192, 1065, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 49_ 8_overlay.mp4\n",
      "[Patient 52] 4 pares vÃ¡lidos\n",
      "(383, 1080, 1854, 3) (383, 1075, 866, 3) (383, 1080, 1804) (383, 1075, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 1_overlay.mp4\n",
      "(370, 1080, 1854, 3) (370, 1076, 860, 3) (370, 1080, 1804) (370, 1076, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 4_overlay.mp4\n",
      "(264, 1080, 1854, 3) (264, 1067, 864, 3) (264, 1080, 1804) (264, 1067, 864)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 6_overlay.mp4\n",
      "(438, 1080, 1854, 3) (438, 1064, 866, 3) (438, 1080, 1804) (438, 1064, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 52_ 8_overlay.mp4\n",
      "[Patient 54] 4 pares vÃ¡lidos\n",
      "(409, 1080, 1854, 3) (409, 1074, 860, 3) (409, 1080, 1804) (409, 1074, 860)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 1_overlay.mp4\n",
      "(337, 1080, 1854, 3) (337, 1077, 862, 3) (337, 1080, 1804) (337, 1077, 862)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 4_overlay.mp4\n",
      "(281, 1080, 1854, 3) (281, 1076, 862, 3) (281, 1080, 1804) (281, 1076, 862)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 6_overlay.mp4\n",
      "(330, 1080, 1854, 3) (330, 1075, 861, 3) (330, 1080, 1804) (330, 1075, 861)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 54_ 7_overlay.mp4\n",
      "[Patient 58] 4 pares vÃ¡lidos\n",
      "(292, 1080, 1854, 3) (292, 426, 822, 3) (292, 1080, 1804) (292, 426, 822)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 2_overlay.mp4\n",
      "(280, 1080, 1854, 3) (280, 1015, 859, 3) (280, 1080, 1804) (280, 1015, 859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (280, 1016, 859) != target shape (280, 1015, 859). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 3_overlay.mp4\n",
      "(251, 1080, 1854, 3) (251, 787, 854, 3) (251, 1080, 1804) (251, 787, 854)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 6_overlay.mp4\n",
      "(526, 1080, 1854, 3) (526, 734, 849, 3) (526, 1080, 1804) (526, 734, 849)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (526, 734, 850) != target shape (526, 734, 849). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 58_ 8_overlay.mp4\n",
      "[Patient 59] 4 pares vÃ¡lidos\n",
      "(280, 1080, 1854, 3) (280, 1066, 866, 3) (280, 1080, 1804) (280, 1066, 866)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 2_overlay.mp4\n",
      "(280, 1080, 1854, 3) (280, 1076, 865, 3) (280, 1080, 1804) (280, 1076, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 4_overlay.mp4\n",
      "(300, 1080, 1854, 3) (300, 1075, 865, 3) (300, 1080, 1804) (300, 1075, 865)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 6_overlay.mp4\n",
      "(244, 1080, 1854, 3) (244, 1065, 859, 3) (244, 1080, 1804) (244, 1065, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 59_ 8_overlay.mp4\n",
      "[Patient 61] 4 pares vÃ¡lidos\n",
      "(295, 1080, 1854, 3) (295, 1076, 859, 3) (295, 1080, 1804) (295, 1076, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 2_overlay.mp4\n",
      "(300, 1080, 1854, 3) (300, 1077, 859, 3) (300, 1080, 1804) (300, 1077, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 3_overlay.mp4\n",
      "(210, 1080, 1854, 3) (210, 316, 758, 3) (210, 1080, 1804) (210, 316, 758)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 5_overlay.mp4\n",
      "(213, 1080, 1854, 3) (213, 680, 807, 3) (213, 1080, 1804) (213, 680, 807)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 61_ 8_overlay.mp4\n",
      "[Patient 62] 4 pares vÃ¡lidos\n",
      "(379, 1080, 1854, 3) (379, 1077, 858, 3) (379, 1080, 1804) (379, 1077, 858)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 1_overlay.mp4\n",
      "(281, 1080, 1854, 3) (281, 891, 861, 3) (281, 1080, 1804) (281, 891, 861)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 4_overlay.mp4\n",
      "(205, 1080, 1854, 3) (205, 799, 859, 3) (205, 1080, 1804) (205, 799, 859)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 6_overlay.mp4\n",
      "(355, 1080, 1854, 3) (355, 207, 698, 3) (355, 1080, 1804) (355, 207, 698)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 62_ 8_overlay.mp4\n",
      "[Patient 63] 4 pares vÃ¡lidos\n",
      "(463, 1080, 854, 3) (463, 708, 690, 3) (463, 1080, 804) (463, 708, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 1_overlay.mp4\n",
      "(458, 1080, 854, 3) (458, 365, 654, 3) (458, 1080, 804) (458, 365, 654)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 4_overlay.mp4\n",
      "(598, 1080, 854, 3) (598, 390, 646, 3) (598, 1080, 804) (598, 390, 646)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 5_overlay.mp4\n",
      "(485, 1080, 854, 3) (485, 432, 661, 3) (485, 1080, 804) (485, 432, 661)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 63_ 8_overlay.mp4\n",
      "[Patient 64] 4 pares vÃ¡lidos\n",
      "(562, 1080, 854, 3) (562, 834, 690, 3) (562, 1080, 804) (562, 834, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (562, 835, 690) != target shape (562, 834, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 1_overlay.mp4\n",
      "(563, 1080, 854, 3) (563, 544, 678, 3) (563, 1080, 804) (563, 544, 678)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 4_overlay.mp4\n",
      "(501, 1080, 854, 3) (501, 746, 689, 3) (501, 1080, 804) (501, 746, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (502, 746, 689) != target shape (501, 746, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 5_overlay.mp4\n",
      "(661, 1080, 854, 3) (661, 696, 688, 3) (661, 1080, 804) (661, 696, 688)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 64_ 8_overlay.mp4\n",
      "[Patient 66] 4 pares vÃ¡lidos\n",
      "(459, 1080, 854, 3) (459, 846, 689, 3) (459, 1080, 804) (459, 846, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 1_overlay.mp4\n",
      "(477, 1080, 854, 3) (477, 844, 690, 3) (477, 1080, 804) (477, 844, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 3_overlay.mp4\n",
      "(422, 1080, 854, 3) (422, 849, 690, 3) (422, 1080, 804) (422, 849, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (422, 850, 690) != target shape (422, 849, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 5_overlay.mp4\n",
      "(485, 1080, 854, 3) (485, 850, 690, 3) (485, 1080, 804) (485, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (485, 851, 690) != target shape (485, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 66_ 8_overlay.mp4\n",
      "[Patient 67a] 4 pares vÃ¡lidos\n",
      "(404, 1080, 854, 3) (404, 844, 690, 3) (404, 1080, 804) (404, 844, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 2_overlay.mp4\n",
      "(345, 1080, 854, 3) (345, 842, 690, 3) (345, 1080, 804) (345, 842, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 4_overlay.mp4\n",
      "(405, 1080, 854, 3) (405, 844, 690, 3) (405, 1080, 804) (405, 844, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 6_overlay.mp4\n",
      "(430, 1080, 854, 3) (430, 844, 689, 3) (430, 1080, 804) (430, 844, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67a_ 7_overlay.mp4\n",
      "[Patient 67b] 4 pares vÃ¡lidos\n",
      "(337, 1080, 854, 3) (337, 844, 690, 3) (337, 1080, 804) (337, 844, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 2_overlay.mp4\n",
      "(392, 1080, 854, 3) (392, 842, 690, 3) (392, 1080, 804) (392, 842, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (393, 842, 690) != target shape (392, 842, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 3_overlay.mp4\n",
      "(373, 1080, 854, 3) (373, 843, 690, 3) (373, 1080, 804) (373, 843, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 5_overlay.mp4\n",
      "(406, 1080, 854, 3) (406, 842, 691, 3) (406, 1080, 804) (406, 842, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 67b_ 8_overlay.mp4\n",
      "[Patient 68] 4 pares vÃ¡lidos\n",
      "(479, 1080, 854, 3) (479, 844, 689, 3) (479, 1080, 804) (479, 844, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (480, 844, 689) != target shape (479, 844, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 1_overlay.mp4\n",
      "(547, 1080, 854, 3) (547, 850, 690, 3) (547, 1080, 804) (547, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (547, 851, 690) != target shape (547, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 3_overlay.mp4\n",
      "(442, 1080, 854, 3) (442, 849, 689, 3) (442, 1080, 804) (442, 849, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (442, 850, 689) != target shape (442, 849, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 5_overlay.mp4\n",
      "(439, 1080, 854, 3) (439, 849, 690, 3) (439, 1080, 804) (439, 849, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (439, 850, 690) != target shape (439, 849, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 68_ 7_overlay.mp4\n",
      "[Patient 69] 4 pares vÃ¡lidos\n",
      "(246, 1080, 854, 3) (246, 850, 690, 3) (246, 1080, 804) (246, 850, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (246, 851, 690) != target shape (246, 850, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 2_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 843, 690, 3) (242, 1080, 804) (242, 843, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 3_overlay.mp4\n",
      "(240, 1080, 854, 3) (240, 845, 691, 3) (240, 1080, 804) (240, 845, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 6_overlay.mp4\n",
      "(236, 1080, 854, 3) (236, 844, 691, 3) (236, 1080, 804) (236, 844, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 69_ 8_overlay.mp4\n",
      "[Patient 70] 4 pares vÃ¡lidos\n",
      "(421, 1080, 858, 3) (421, 855, 771, 3) (421, 1080, 808) (421, 855, 771)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 2_overlay.mp4\n",
      "(355, 1080, 858, 3) (355, 855, 692, 3) (355, 1080, 808) (355, 855, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 3_overlay.mp4\n",
      "(432, 1080, 858, 3) (432, 856, 728, 3) (432, 1080, 808) (432, 856, 728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (432, 857, 728) != target shape (432, 856, 728). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 6_overlay.mp4\n",
      "(544, 1080, 858, 3) (544, 857, 808, 3) (544, 1080, 808) (544, 857, 808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (544, 858, 808) != target shape (544, 857, 808). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 70_ 7_overlay.mp4\n",
      "[Patient 71] 4 pares vÃ¡lidos\n",
      "(464, 1080, 858, 3) (464, 854, 697, 3) (464, 1080, 808) (464, 854, 697)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 2_overlay.mp4\n",
      "(539, 1080, 858, 3) (539, 851, 692, 3) (539, 1080, 808) (539, 851, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 3_overlay.mp4\n",
      "(672, 1080, 858, 3) (672, 855, 694, 3) (672, 1080, 808) (672, 855, 694)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 5_overlay.mp4\n",
      "(605, 1080, 858, 3) (605, 853, 693, 3) (605, 1080, 808) (605, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (605, 854, 693) != target shape (605, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 71_ 7_overlay.mp4\n",
      "[Patient 73] 4 pares vÃ¡lidos\n",
      "(427, 1080, 854, 3) (427, 842, 689, 3) (427, 1080, 804) (427, 842, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 1_overlay.mp4\n",
      "(600, 1080, 854, 3) (600, 832, 687, 3) (600, 1080, 804) (600, 832, 687)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 4_overlay.mp4\n",
      "(576, 1080, 854, 3) (576, 658, 687, 3) (576, 1080, 804) (576, 658, 687)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 6_overlay.mp4\n",
      "(448, 1080, 854, 3) (448, 834, 688, 3) (448, 1080, 804) (448, 834, 688)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (448, 835, 688) != target shape (448, 834, 688). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 73_ 8_overlay.mp4\n",
      "[Patient 76] 4 pares vÃ¡lidos\n",
      "(217, 1080, 854, 3) (217, 840, 691, 3) (217, 1080, 804) (217, 840, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 1_overlay.mp4\n",
      "(244, 1080, 854, 3) (244, 844, 689, 3) (244, 1080, 804) (244, 844, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 4_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 844, 689, 3) (242, 1080, 804) (242, 844, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 5_overlay.mp4\n",
      "(240, 1080, 854, 3) (240, 843, 690, 3) (240, 1080, 804) (240, 843, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 76_ 7_overlay.mp4\n",
      "[Patient 78a] 4 pares vÃ¡lidos\n",
      "(466, 1080, 858, 3) (466, 849, 692, 3) (466, 1080, 808) (466, 849, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (466, 850, 692) != target shape (466, 849, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 1_overlay.mp4\n",
      "(553, 1080, 858, 3) (553, 852, 692, 3) (553, 1080, 808) (553, 852, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 4_overlay.mp4\n",
      "(488, 1080, 858, 3) (488, 855, 730, 3) (488, 1080, 808) (488, 855, 730)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 5_overlay.mp4\n",
      "(460, 1080, 858, 3) (460, 853, 691, 3) (460, 1080, 808) (460, 853, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (460, 854, 691) != target shape (460, 853, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78a_ 7_overlay.mp4\n",
      "[Patient 78b] 3 pares vÃ¡lidos\n",
      "(559, 1080, 858, 3) (559, 853, 729, 3) (559, 1080, 808) (559, 853, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (559, 854, 729) != target shape (559, 853, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 3_overlay.mp4\n",
      "(482, 1080, 858, 3) (482, 855, 691, 3) (482, 1080, 808) (482, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 6_overlay.mp4\n",
      "(641, 1080, 858, 3) (641, 855, 729, 3) (641, 1080, 808) (641, 855, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 78b_ 8_overlay.mp4\n",
      "[Patient 82] 2 pares vÃ¡lidos\n",
      "(275, 1080, 858, 3) (275, 848, 691, 3) (275, 1080, 808) (275, 848, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 82_ 2_overlay.mp4\n",
      "(284, 1080, 858, 3) (284, 850, 692, 3) (284, 1080, 808) (284, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (284, 851, 692) != target shape (284, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 82_ 8_overlay.mp4\n",
      "[Patient 83] 4 pares vÃ¡lidos\n",
      "(391, 1080, 858, 3) (391, 854, 691, 3) (391, 1080, 808) (391, 854, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 1_overlay.mp4\n",
      "(507, 1080, 858, 3) (507, 784, 727, 3) (507, 1080, 808) (507, 784, 727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (507, 785, 727) != target shape (507, 784, 727). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 3_overlay.mp4\n",
      "(590, 1080, 858, 3) (590, 849, 772, 3) (590, 1080, 808) (590, 849, 772)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (590, 850, 772) != target shape (590, 849, 772). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 6_overlay.mp4\n",
      "(689, 1080, 858, 3) (689, 856, 692, 3) (689, 1080, 808) (689, 856, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (689, 857, 692) != target shape (689, 856, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 83_ 7_overlay.mp4\n",
      "[Patient 84] 4 pares vÃ¡lidos\n",
      "(331, 1080, 854, 3) (331, 830, 690, 3) (331, 1080, 804) (331, 830, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 2_overlay.mp4\n",
      "(267, 1080, 854, 3) (267, 843, 691, 3) (267, 1080, 804) (267, 843, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 3_overlay.mp4\n",
      "(312, 1080, 854, 3) (312, 815, 690, 3) (312, 1080, 804) (312, 815, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 6_overlay.mp4\n",
      "(242, 1080, 854, 3) (242, 841, 690, 3) (242, 1080, 804) (242, 841, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (242, 842, 690) != target shape (242, 841, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 84_ 7_overlay.mp4\n",
      "[Patient 85] 4 pares vÃ¡lidos\n",
      "(496, 1080, 858, 3) (496, 854, 691, 3) (496, 1080, 808) (496, 854, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 1_overlay.mp4\n",
      "(714, 1080, 858, 3) (714, 853, 693, 3) (714, 1080, 808) (714, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (714, 854, 693) != target shape (714, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 4_overlay.mp4\n",
      "(706, 1080, 858, 3) (706, 855, 694, 3) (706, 1080, 808) (706, 855, 694)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 5_overlay.mp4\n",
      "(684, 1080, 858, 3) (684, 851, 729, 3) (684, 1080, 808) (684, 851, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 85_ 7_overlay.mp4\n",
      "[Patient 87] 4 pares vÃ¡lidos\n",
      "(325, 1080, 858, 3) (325, 853, 690, 3) (325, 1080, 808) (325, 853, 690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (325, 854, 690) != target shape (325, 853, 690). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 1_overlay.mp4\n",
      "(272, 1080, 858, 3) (272, 855, 691, 3) (272, 1080, 808) (272, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 4_overlay.mp4\n",
      "(282, 1080, 858, 3) (282, 857, 691, 3) (282, 1080, 808) (282, 857, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (282, 858, 691) != target shape (282, 857, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 5_overlay.mp4\n",
      "(276, 1080, 858, 3) (276, 851, 691, 3) (276, 1080, 808) (276, 851, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 87_ 7_overlay.mp4\n",
      "[Patient 88] 4 pares vÃ¡lidos\n",
      "(434, 1080, 858, 3) (434, 844, 692, 3) (434, 1080, 808) (434, 844, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 1_overlay.mp4\n",
      "(439, 1080, 858, 3) (439, 851, 692, 3) (439, 1080, 808) (439, 851, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 4_overlay.mp4\n",
      "(470, 1080, 858, 3) (470, 850, 692, 3) (470, 1080, 808) (470, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (470, 851, 692) != target shape (470, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 6_overlay.mp4\n",
      "(449, 1080, 858, 3) (449, 847, 690, 3) (449, 1080, 808) (449, 847, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 88_ 7_overlay.mp4\n",
      "[Patient 89] 4 pares vÃ¡lidos\n",
      "(605, 1080, 858, 3) (605, 854, 692, 3) (605, 1080, 808) (605, 854, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 1_overlay.mp4\n",
      "(764, 1080, 858, 3) (764, 856, 808, 3) (764, 1080, 808) (764, 856, 808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (764, 857, 808) != target shape (764, 856, 808). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 4_overlay.mp4\n",
      "(744, 1080, 858, 3) (744, 852, 691, 3) (744, 1080, 808) (744, 852, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 6_overlay.mp4\n",
      "(670, 1080, 858, 3) (670, 858, 692, 3) (670, 1080, 808) (670, 858, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 89_ 7_overlay.mp4\n",
      "[Patient 92] 4 pares vÃ¡lidos\n",
      "(259, 1080, 858, 3) (259, 851, 694, 3) (259, 1080, 808) (259, 851, 694)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 1_overlay.mp4\n",
      "(252, 1080, 858, 3) (252, 857, 691, 3) (252, 1080, 808) (252, 857, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (252, 858, 691) != target shape (252, 857, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 4_overlay.mp4\n",
      "(239, 1080, 858, 3) (239, 856, 729, 3) (239, 1080, 808) (239, 856, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 857, 729) != target shape (239, 856, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 6_overlay.mp4\n",
      "(249, 1080, 858, 3) (249, 854, 692, 3) (249, 1080, 808) (249, 854, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (250, 854, 692) != target shape (249, 854, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 92_ 8_overlay.mp4\n",
      "[Patient 93] 4 pares vÃ¡lidos\n",
      "(571, 1080, 858, 3) (571, 855, 691, 3) (571, 1080, 808) (571, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 2_overlay.mp4\n",
      "(386, 1080, 858, 3) (386, 851, 691, 3) (386, 1080, 808) (386, 851, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 4_overlay.mp4\n",
      "(392, 1080, 858, 3) (392, 848, 729, 3) (392, 1080, 808) (392, 848, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (393, 848, 729) != target shape (392, 848, 729). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 6_overlay.mp4\n",
      "(598, 1080, 858, 3) (598, 851, 693, 3) (598, 1080, 808) (598, 851, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 93_ 7_overlay.mp4\n",
      "[Patient 94a] 4 pares vÃ¡lidos\n",
      "(430, 1080, 858, 3) (430, 700, 729, 3) (430, 1080, 808) (430, 700, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 1_overlay.mp4\n",
      "(380, 1080, 858, 3) (380, 653, 690, 3) (380, 1080, 808) (380, 653, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 3_overlay.mp4\n",
      "(423, 1080, 858, 3) (423, 669, 770, 3) (423, 1080, 808) (423, 669, 770)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 6_overlay.mp4\n",
      "(448, 1080, 858, 3) (448, 745, 729, 3) (448, 1080, 808) (448, 745, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 94a_ 7_overlay.mp4\n",
      "[Patient 95] 4 pares vÃ¡lidos\n",
      "(244, 1080, 858, 3) (244, 862, 693, 3) (244, 1080, 808) (244, 862, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 1_overlay.mp4\n",
      "(286, 1080, 858, 3) (286, 854, 691, 3) (286, 1080, 808) (286, 854, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 4_overlay.mp4\n",
      "(237, 1080, 858, 3) (237, 855, 691, 3) (237, 1080, 808) (237, 855, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 855, 691) != target shape (237, 855, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 5_overlay.mp4\n",
      "(312, 1080, 858, 3) (312, 858, 767, 3) (312, 1080, 808) (312, 858, 767)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 95_ 7_overlay.mp4\n",
      "[Patient 96] 4 pares vÃ¡lidos\n",
      "(468, 1080, 858, 3) (468, 851, 691, 3) (468, 1080, 808) (468, 851, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 2_overlay.mp4\n",
      "(483, 1080, 858, 3) (483, 632, 686, 3) (483, 1080, 808) (483, 632, 686)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 4_overlay.mp4\n",
      "(540, 1080, 858, 3) (540, 847, 690, 3) (540, 1080, 808) (540, 847, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 5_overlay.mp4\n",
      "(454, 1080, 858, 3) (454, 792, 808, 3) (454, 1080, 808) (454, 792, 808)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 96_ 7_overlay.mp4\n",
      "[Patient 97] 4 pares vÃ¡lidos\n",
      "(459, 1080, 858, 3) (459, 849, 753, 3) (562, 1080, 808) (562, 849, 753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (562, 850, 753) != target shape (562, 849, 753). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Overlay fallo subject=Patient 97 idx=1: \n",
      "(545, 1080, 858, 3) (545, 751, 771, 3) (545, 1080, 808) (545, 751, 771)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 2_overlay.mp4\n",
      "(602, 1080, 858, 3) (602, 787, 690, 3) (602, 1080, 808) (602, 787, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 4_overlay.mp4\n",
      "(526, 1080, 858, 3) (526, 850, 691, 3) (526, 1080, 808) (526, 850, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (526, 851, 691) != target shape (526, 850, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 97_ 8_overlay.mp4\n",
      "[Patient 98] 4 pares vÃ¡lidos\n",
      "(284, 1080, 858, 3) (284, 856, 693, 3) (284, 1080, 808) (284, 856, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (284, 857, 693) != target shape (284, 856, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 1_overlay.mp4\n",
      "(282, 1080, 858, 3) (282, 855, 691, 3) (282, 1080, 808) (282, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 4_overlay.mp4\n",
      "(238, 1080, 858, 3) (238, 862, 693, 3) (238, 1080, 808) (238, 862, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 5_overlay.mp4\n",
      "(290, 1080, 858, 3) (290, 856, 691, 3) (290, 1080, 808) (290, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (290, 857, 691) != target shape (290, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 98_ 7_overlay.mp4\n",
      "[Patient 99] 4 pares vÃ¡lidos\n",
      "(267, 1080, 858, 3) (267, 852, 693, 3) (267, 1080, 808) (267, 852, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 1_overlay.mp4\n",
      "(259, 1080, 858, 3) (259, 853, 692, 3) (259, 1080, 808) (259, 853, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (259, 854, 692) != target shape (259, 853, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 3_overlay.mp4\n",
      "(248, 1080, 858, 3) (248, 856, 691, 3) (248, 1080, 808) (248, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (248, 857, 691) != target shape (248, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 5_overlay.mp4\n",
      "(343, 1080, 858, 3) (343, 855, 691, 3) (343, 1080, 808) (343, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 99_ 8_overlay.mp4\n",
      "[Patient 102] 4 pares vÃ¡lidos\n",
      "(270, 1080, 858, 3) (270, 804, 686, 3) (270, 1080, 808) (270, 804, 686)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 2_overlay.mp4\n",
      "(253, 1080, 858, 3) (253, 854, 691, 3) (253, 1080, 808) (253, 854, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (254, 854, 691) != target shape (253, 854, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 3_overlay.mp4\n",
      "(244, 1080, 858, 3) (244, 852, 689, 3) (244, 1080, 808) (244, 852, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 5_overlay.mp4\n",
      "(237, 1080, 858, 3) (237, 850, 692, 3) (237, 1080, 808) (237, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 851, 692) != target shape (237, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 102_ 7_overlay.mp4\n",
      "[Patient 103] 4 pares vÃ¡lidos\n",
      "(402, 1080, 858, 3) (402, 851, 692, 3) (402, 1080, 808) (402, 851, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 1_overlay.mp4\n",
      "(622, 1080, 858, 3) (622, 850, 692, 3) (622, 1080, 808) (622, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (622, 851, 692) != target shape (622, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 3_overlay.mp4\n",
      "(557, 1080, 858, 3) (557, 856, 692, 3) (557, 1080, 808) (557, 856, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (557, 857, 692) != target shape (557, 856, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 5_overlay.mp4\n",
      "(607, 1080, 858, 3) (607, 854, 689, 3) (607, 1080, 808) (607, 854, 689)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 103_ 8_overlay.mp4\n",
      "[Patient 104] 1 pares vÃ¡lidos\n",
      "(681, 1080, 858, 3) (681, 855, 691, 3) (681, 1080, 808) (681, 855, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 104_ 7_overlay.mp4\n",
      "[Patient 106] 4 pares vÃ¡lidos\n",
      "(221, 1080, 858, 3) (221, 859, 729, 3) (221, 1080, 808) (221, 859, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 2_overlay.mp4\n",
      "(246, 1080, 858, 3) (246, 858, 696, 3) (246, 1080, 808) (246, 858, 696)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 4_overlay.mp4\n",
      "(266, 1080, 858, 3) (266, 856, 691, 3) (266, 1080, 808) (266, 856, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (266, 857, 691) != target shape (266, 856, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 6_overlay.mp4\n",
      "(239, 1080, 858, 3) (239, 855, 764, 3) (239, 1080, 808) (239, 855, 764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (240, 855, 764) != target shape (239, 855, 764). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 106_ 8_overlay.mp4\n",
      "[Patient 108] 4 pares vÃ¡lidos\n",
      "(737, 1080, 858, 3) (737, 786, 671, 3) (737, 1080, 808) (737, 786, 671)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (738, 786, 671) != target shape (737, 786, 671). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 2_overlay.mp4\n",
      "(609, 1080, 858, 3) (609, 843, 690, 3) (609, 1080, 808) (609, 843, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 4_overlay.mp4\n",
      "(772, 1080, 858, 3) (772, 838, 690, 3) (772, 1080, 808) (772, 838, 690)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 6_overlay.mp4\n",
      "(670, 1080, 858, 3) (670, 747, 808, 3) (670, 1080, 808) (670, 747, 808)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 108_ 8_overlay.mp4\n",
      "[Patient 110] 1 pares vÃ¡lidos\n",
      "(203, 1080, 858, 3) (203, 850, 692, 3) (203, 1080, 808) (203, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (203, 851, 692) != target shape (203, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 110_ 5_overlay.mp4\n",
      "[Patient 111] 4 pares vÃ¡lidos\n",
      "(196, 1080, 858, 3) (196, 855, 692, 3) (196, 1080, 808) (196, 855, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (197, 855, 692) != target shape (196, 855, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 2_overlay.mp4\n",
      "(208, 1080, 858, 3) (208, 858, 691, 3) (208, 1080, 808) (208, 858, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 4_overlay.mp4\n",
      "(173, 1080, 858, 3) (173, 853, 693, 3) (173, 1080, 808) (173, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (173, 854, 693) != target shape (173, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 5_overlay.mp4\n",
      "(245, 1080, 858, 3) (245, 860, 691, 3) (245, 1080, 808) (245, 860, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 111_ 7_overlay.mp4\n",
      "[Patient 112] 4 pares vÃ¡lidos\n",
      "(547, 1080, 858, 3) (547, 855, 693, 3) (547, 1080, 808) (547, 855, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 2_overlay.mp4\n",
      "(624, 1080, 858, 3) (624, 850, 692, 3) (624, 1080, 808) (624, 850, 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (624, 851, 692) != target shape (624, 850, 692). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 4_overlay.mp4\n",
      "(533, 1080, 858, 3) (533, 854, 693, 3) (533, 1080, 808) (533, 854, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 5_overlay.mp4\n",
      "(618, 1080, 858, 3) (618, 855, 693, 3) (618, 1080, 808) (618, 855, 693)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 112_ 8_overlay.mp4\n",
      "[Patient 113] 4 pares vÃ¡lidos\n",
      "(237, 1080, 858, 3) (237, 854, 691, 3) (237, 1080, 808) (237, 854, 691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (238, 854, 691) != target shape (237, 854, 691). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 2_overlay.mp4\n",
      "(293, 1080, 858, 3) (293, 855, 692, 3) (293, 1080, 808) (293, 855, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 4_overlay.mp4\n",
      "(235, 1080, 858, 3) (235, 856, 689, 3) (235, 1080, 808) (235, 856, 689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (235, 857, 689) != target shape (235, 856, 689). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 6_overlay.mp4\n",
      "(285, 1080, 858, 3) (285, 851, 691, 3) (285, 1080, 808) (285, 851, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 113_ 8_overlay.mp4\n",
      "[Patient 115a] 4 pares vÃ¡lidos\n",
      "(259, 1080, 858, 3) (259, 851, 692, 3) (259, 1080, 808) (259, 851, 692)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 2_overlay.mp4\n",
      "(209, 1080, 858, 3) (209, 855, 764, 3) (209, 1080, 808) (209, 855, 764)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 3_overlay.mp4\n",
      "(187, 1080, 858, 3) (187, 855, 729, 3) (187, 1080, 808) (187, 855, 729)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 5_overlay.mp4\n",
      "(241, 1080, 858, 3) (241, 853, 693, 3) (241, 1080, 808) (241, 853, 693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/ligthning/lib/python3.12/site-packages/torchio/transforms/transform.py:162: RuntimeWarning: Output shape (241, 854, 693) != target shape (241, 853, 693). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 115a_ 8_overlay.mp4\n",
      "[Patient 117] 4 pares vÃ¡lidos\n",
      "(446, 1080, 858, 3) (446, 855, 694, 3) (446, 1080, 808) (446, 855, 694)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 2_overlay.mp4\n",
      "(504, 1080, 858, 3) (504, 859, 691, 3) (504, 1080, 808) (504, 859, 691)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 4_overlay.mp4\n",
      "(614, 1080, 858, 3) (614, 855, 761, 3) (614, 1080, 808) (614, 855, 761)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 5_overlay.mp4\n",
      "(526, 1080, 858, 3) (526, 855, 808, 3) (526, 1080, 808) (526, 855, 808)\n",
      "ðŸŽ¥ guardado: /data/GitHub/Breast-AI-model/src/overlay_test_rc2/Patient 117_ 8_overlay.mp4\n",
      "Total patients processed: 65\n",
      "Total raw patients: 107\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import natsort\n",
    "import numpy as np\n",
    "from torchmetrics import Dice, JaccardIndex, Precision, Recall, Specificity, Accuracy\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import skvideo.io as sio\n",
    "import random\n",
    "import torchio as tio\n",
    "import skimage.morphology as skm\n",
    "from skimage import measure\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from miseval import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "\n",
    "\n",
    "def draw_label(img_rgb, text, org, font_scale=0.8, color=(255,255,255)):\n",
    "    cv2.putText(img_rgb, text, org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(img_rgb, text, org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def overlay_mask_on_frame(frame_rgb, mask_2d, color=(0,255,0), alpha=0.35, draw_contour=True):\n",
    "    out = frame_rgb.copy()\n",
    "    m = mask_2d.astype(bool)\n",
    "    if m.any():\n",
    "        color_arr = np.array(color, dtype=np.uint8)\n",
    "        out[m] = (out[m] * (1 - alpha) + color_arr * alpha).astype(np.uint8)\n",
    "        if draw_contour:\n",
    "            edges = find_boundaries(mask_2d, mode=\"inner\")\n",
    "            out[edges] = color_arr\n",
    "    return out\n",
    "\n",
    "def make_even_frame(img_rgb):\n",
    "    \"\"\"Asegura H y W pares para yuv420p con padding de 1 px si hace falta.\"\"\"\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    if pad_h or pad_w:\n",
    "        img_rgb = np.pad(img_rgb, ((0, pad_h), (0, pad_w), (0, 0)), mode='edge')\n",
    "    # garantizar memoria contigua y dtype correcto\n",
    "    return np.ascontiguousarray(img_rgb.astype(np.uint8))\n",
    "\n",
    "def write_triptych_skvideo(orig_DHWC, gt_DHW, pred_DHW, out_path, fps=25.0, method=\"ffmpegwriter\"):\n",
    "    D, H, W, _ = orig_DHWC.shape\n",
    "    assert gt_DHW.shape == (D, H, W) and pred_DHW.shape == (D, H, W)\n",
    "\n",
    "    if method == \"vwrite\":\n",
    "        frames = []\n",
    "        for t in range(D):\n",
    "            f0 = orig_DHWC[t]\n",
    "            f1 = overlay_mask_on_frame(f0, gt_DHW[t],  color=(0,255,0))\n",
    "            f2 = overlay_mask_on_frame(f0, pred_DHW[t], color=(255,0,255))\n",
    "            trip = np.concatenate([f0, f1, f2], axis=1)\n",
    "            draw_label(trip, \"Original\", (15, 34))\n",
    "            draw_label(trip, \"GT\",       (15 + W, 34))\n",
    "            draw_label(trip, \"Pred\",     (15 + 2*W, 34))\n",
    "            trip = make_even_frame(trip)\n",
    "            frames.append(trip)\n",
    "        vid = np.stack(frames, axis=0)\n",
    "        sio.vwrite(out_path, vid, outputdict={'-r': str(fps), '-vcodec': 'libx264', '-pix_fmt': 'yuv420p'})\n",
    "    else:\n",
    "        writer = sio.FFmpegWriter(out_path, outputdict={'-r': str(fps), '-vcodec': 'libx264', '-pix_fmt': 'yuv420p'})\n",
    "        for t in range(D):\n",
    "            f0 = orig_DHWC[t]\n",
    "            f1 = overlay_mask_on_frame(f0, gt_DHW[t],  color=(0,255,0))\n",
    "            f2 = overlay_mask_on_frame(f0, pred_DHW[t], color=(255,0,255))\n",
    "            trip = np.concatenate([f0, f1, f2], axis=1)\n",
    "            draw_label(trip, \"Original\", (15, 34))\n",
    "            draw_label(trip, \"GT\",       (15 + W, 34))\n",
    "            draw_label(trip, \"Pred\",     (15 + 2*W, 34))\n",
    "            trip = make_even_frame(trip)\n",
    "            writer.writeFrame(trip)\n",
    "        writer.close()\n",
    "    print(f\"ðŸŽ¥ guardado: {out_path}\")\n",
    "\n",
    "# === Resize de label map con NN a 128Â³ ===\n",
    "def resize_pred_n(label_vol_DHW,D, H, W):\n",
    "    # TorchIO espera (C, D, H, W); usamos C=1\n",
    "    lbl = label_vol_DHW.astype(np.int8)  # o uint8/uint16 segÃºn tus clases\n",
    "    img = tio.LabelMap(tensor=lbl[None, ...])  # (1, D, H, W)\n",
    "    img = tio.Resize((D,H,W))(img)     # NN por ser LabelMap\n",
    "    out = img.data.numpy()[0]                  # (D, H, W)\n",
    "    return out\n",
    "\n",
    "\n",
    "output_dir = \"/data/GitHub/Breast-AI-model/src/overlay_test_rc2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def safe_eval(gt, pred, metric):\n",
    "    try:\n",
    "        return evaluate(gt, pred, metric=metric)\n",
    "    except Exception as e:\n",
    "        # opcional: print(f\"[WARN] {metric} fallÃ³: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Acumuladores\n",
    "per_video_rows = []\n",
    "\n",
    "subfolders_ids = natsort.natsorted(get_subfolder_ids('/data/GitHub/Breast-AI-model/src/predictions_rc2/'))\n",
    "count_patients = 0\n",
    "\n",
    "for subfolder_id in subfolders_ids:\n",
    "    ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'FINAL LABELS')\n",
    "    if not os.path.exists(ground_truth_folder):\n",
    "        ground_truth_folder = os.path.join(ground_truth_path, subfolder_id, 'Segmentations')\n",
    "        if not os.path.exists(ground_truth_folder):\n",
    "            continue\n",
    "    count_patients += 1\n",
    "\n",
    "    ground_truth_files = glob.glob(os.path.join(ground_truth_folder, '**', '*.mat'), recursive=True)\n",
    "    ground_truth_files = natsort.natsorted([f for f in ground_truth_files\n",
    "                                            if 'groundTruthMed.mat' not in f and 'Session.mat' not in f])\n",
    "\n",
    "    pred_files = glob.glob(os.path.join('/data/GitHub/Breast-AI-model/src/predictions_rc2', subfolder_id, '*.npy'))\n",
    "    pred_files = natsort.natsorted(pred_files)\n",
    "\n",
    "    patterns = [\n",
    "    os.path.join(videos_path, subfolder_id, 'VSI 8 [cC]lips', '*.[mM][pP]4'),\n",
    "    os.path.join(videos_path, subfolder_id, '8 VSI [cC]lips', '*.[mM][pP]4'),  # opcional, por si existe esa variante\n",
    "]\n",
    "\n",
    "    videos_files = []\n",
    "    for pat in patterns:\n",
    "        videos_files += glob.glob(pat)\n",
    "\n",
    "    # quitar duplicados y ordenar de forma natural\n",
    "    videos_files = natsort.natsorted({os.path.realpath(p) for p in videos_files})\n",
    "    # idx -> path\n",
    "    npy_by_idx = {get_npy_idx(p): p for p in pred_files if get_npy_idx(p) is not None}\n",
    "    videos_by_idx = {get_video_idx(v): v for v in videos_files if get_video_idx(v) is not None}\n",
    "\n",
    "    # (gt_mat, pred_npy, video_mp4, idx)\n",
    "    pairs = []\n",
    "    for mp in ground_truth_files:\n",
    "        i = get_mat_idx(mp)\n",
    "        if i is None:\n",
    "            continue\n",
    "        pred_p = npy_by_idx.get(i)\n",
    "        vid_p  = videos_by_idx.get(i)\n",
    "        if pred_p is not None and vid_p is not None:   # si no hay video, omitimos porque el ROI depende del video\n",
    "            pairs.append((mp, pred_p, vid_p, i))\n",
    "\n",
    "    print(f\"[{subfolder_id}] {len(pairs)} pares vÃ¡lidos\")\n",
    "\n",
    "    # ---- MÃ©tricas por video ----\n",
    "    # Por cada par\n",
    "    for gt_file, pred_file, vid_file, idx in pairs:\n",
    "        try:\n",
    "            video = sio.vread(vid_file)  # (D,H,W,3)\n",
    "            \n",
    "            cropped_video, roi_coords = process_video_and_get_crop(video)      # (D',H',W',3)\n",
    "            gt_DHW = np.transpose(loadmat(gt_file)['labels'], (2,0,1))      # (D,H,W)\n",
    "            minr, maxr, minc, maxc = roi_coords\n",
    "            trim_right_px=50\n",
    "\n",
    "            # 2) Aplicar recorte de borde derecho tambiÃ©n al GT\n",
    "            if trim_right_px and gt_DHW.shape[2] > trim_right_px:\n",
    "                gt_DHW = gt_DHW[:, :, 0:-trim_right_px]\n",
    "\n",
    "            # 3) Recortar GT con el mismo ROI del video\n",
    "            gt_bin = gt_DHW[:, minr:maxr, minc:maxc]\n",
    "\n",
    "            print(video.shape, cropped_video.shape, gt_DHW.shape, gt_bin.shape)\n",
    "\n",
    "\n",
    "            pred = np.load(pred_file)                                   # (128,128,128)\n",
    "            pred_bin = (pred > 0).astype(np.uint8)\n",
    "            pred_rs  = resize_pred_n(pred_bin, gt_bin.shape[0], gt_bin.shape[1], gt_bin.shape[2])   # (D',H',W')\n",
    "\n",
    "            fps = get_fps(vid_file, fallback=25.0)\n",
    "            out_name = f\"{subfolder_id}_{idx:2d}_overlay.mp4\"\n",
    "            out_path = os.path.join(output_dir, out_name)\n",
    "\n",
    "            # Usa vwrite si lo quieres explÃ­cito: method=\"vwrite\"\n",
    "            write_triptych_skvideo(cropped_video, gt_bin, pred_rs, out_path, fps=fps, method=\"ffmpegwriter\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Overlay fallo subject={subfolder_id} idx={idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "print(f\"Total patients processed: {count_patients}\")\n",
    "print(f\"Total raw patients: {len(subfolders_ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligthning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
